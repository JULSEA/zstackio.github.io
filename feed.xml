<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ZStack</title>
    <description>ZStack is open source IaaS software managing resources of compute, storage, networking throughout a datacenter all by APIs.</description>
    <link>http://zstack.org/</link>
    <atom:link href="http://zstack.org/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 24 Sep 2015 21:35:49 +0800</pubDate>
    <lastBuildDate>Thu, 24 Sep 2015 21:35:49 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>【免安装】ZStack v0.9三层网络VMWare镜像 </title>
        <description>&lt;p&gt;zstack.tw社区最近制作了ZStack 0.9 VMware Workstation镜像并上传到百度云盘。感兴趣试用的同学可以按照下面的介绍下载试用。&lt;/p&gt;

&lt;p&gt;本次镜像里按照了ZStack 0.9，并且配置了复杂的三层网络模式 (Three Tiered Network模式)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ovf档案百度云盘下载位置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;http://pan.baidu.com/s/1pJmqR8n&lt;/p&gt;

&lt;p&gt;配置说明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;操作系统centos7，预设帐号root，默认密码zstack&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;网络模式是NAT模式，默认ip位置是192.168.230.140&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;开机后记得启动zstack node服务与ui ｀zstack-ctl start_node;zstack-ctl start_ui｀&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;已经下载安装了两个VM的qcow2文件： ttylinux.qcow2， zstack-virtualrouter-0.9.0.qcow2&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; 镜像所在目录：&lt;/p&gt;

&lt;p&gt; /var/www/html/qcow2&lt;/p&gt;

&lt;p&gt; url位置：&lt;/p&gt;

&lt;p&gt; http://192.168.230.140/qcow2&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Linux安装ISO的文件也已下载，共有两个ISO档案，练习安装虚拟机会用到：&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; CentOS-7-x86_64-Minimal-1503-01.iso&lt;/p&gt;

&lt;p&gt; ubuntu-14.04.3-server-amd64.iso&lt;/p&gt;

&lt;p&gt; 文件位置：&lt;/p&gt;

&lt;p&gt; /var/www/html/iso&lt;/p&gt;

&lt;p&gt; url位置：&lt;/p&gt;

&lt;p&gt; http://192.168.230.140/iso&lt;/p&gt;

&lt;p&gt;如果遇到安装使用的问题，大家可以到ZStack QQ群：410185063里需求帮助。&lt;/p&gt;
</description>
        <pubDate>Thu, 24 Sep 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn_blog/zstack-on-vmware-image.html</link>
        <guid isPermaLink="true">http://zstack.org/cn_blog/zstack-on-vmware-image.html</guid>
        
        
        <category>cn_blog</category>
        
      </item>
    
      <item>
        <title>ZStack 虚拟路由器工作流程</title>
        <description>&lt;p&gt;Virtual Router（VR，虚拟路由器）是ZStack中一个特别的网络组件。目前ZStack里大部分的网络服务都是由VR提供的。
VR实际上是一个用于提供服务的特殊虚拟机，
它只在用户虚拟机需要网络服务的时候，由ZStack自动创建和管理。今天我们介绍一下ZStack网络服务里VR的工作流程。
当了解该工作流程后，用户可以更好的规划云环境里的网络服务。一旦出现VR创建失败的情况，也可以更快的定位和解决问题。&lt;/p&gt;

&lt;p&gt;&lt;img  class=&quot;img-responsive&quot;  src=&quot;/images/blogs/misc/zstack-create-vm-process.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;首先来看一下上图，这个网络环境是ZStack教程里典型内外网分离的网络模型，可用于EIP，Port Forwarding，Security Group，Load Balancer等。
在这个模型里，我们有两台物理机器，左边一台是计算节点，用于创建各种虚拟机；右边一台用于安装ZStack管控节点（Management Node）。
计算节点有两块物理网卡eth0和eth1（如果用户只有一个网卡eth0，可以通过vlan创建一个eth0.x的网卡替代eth1）。
计算节点的eth0和管控节点的网卡相连，这个网络在ZStack里面被称为管理网络（Management-l2）。该网络可以直接连接到公网（Internet或者公司内网）。
在该模型中，公有网络（Public-l2）和Management-l2公用一个eth0。当然为了更好的网络隔离，用户也可以再增加一个独立等网卡eth2，
用于Public-l2（由于管控节点控制计算节点不需要走公有网络，通过网络分离，也可以保护管控节点的安全）。
在我们的例子中，我们需要确保ZStack管控节点可以正确的连接计算节点的eth0（图中的10.0.0.1/24网段，我们需要给网卡配置上正确的IP地址）。
这样在添加计算节点的时候，ZStack就可以通过计算节点eth0上的IP地址部署对应的Agent。
由于Public-l2和Management-l2共享了一个L2，所以在配置网络环境的时候，我们只需要添加Public-l2和Public-l3。
随后在创建VR的时候，管控节点会和VR上的eth0进行通讯，VR的eth0的IP地址将会从用户设置的Public-l3中的IP Range取出一个。
在本例子里面，Publice-l3的IP Range一定是在10.0.0.1/24中的某一段。用户在设置Public-l3的IP Range的时候必须确保设置的IP地址段
不和已有网络中的网络设备冲突，Gateway和NetMask也需要设置正确。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：当用户只使用了单台物理机做ZStack部署，并且单台物理机的eth0是从DHCP拿到的IP地址的时候，
如果把eth0作为Public-l2的网卡设备，需要特别关注ZStack给VR分配的IP地址不能和网络中的其他IP冲突。
如果使用没有连接任何网络的ethX作为单节点的Public-l2的网络设备，那么需要在添加L2 Network之前，给ethX设置上相应的IP地址，否则之后无法连通VR的eth0。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在我们的模型里，计算节点上还有一个eth1用于用户VM的私有网络。该eth1可以通过交换机和同一个Cluster内的其他计算节点的eth1相连。
当ZStack添加计算节点的时候，会通过Ansible把KVM Agent安装到计算节点上。当ZStack添加Publice-L2和Private-L2的时候，
会在计算节点上给eth0和eth1创建对应的网桥br_eth0和br_eth1。根据网桥工作原理，ZStack还会把eth0和eth1上的IP地址转移到br_eth0和br_eth1上。&lt;/p&gt;

&lt;p&gt;添加完成所有的资源，用户在创建第一个VM的时候，如果用户VM使用了VR的任何一种网络服务（DHCP、DNS、SNAT、Port Forwarding、EIP、Load Balancer），
ZStack会自动创建一个VR VM。这也是创建第一个VM的时候速度比较慢的原因之一。&lt;/p&gt;

&lt;p&gt;ZStack创建VR VM的过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ZStack 根据 VR 模版创建一个VR VM，该VM有两个网卡，一个连接到br_eth0负责连接管控节点和公网（如果有独立的Public-l2，会多创建一个网卡），另一个连接到br_eth1负责连接用户VM。&lt;/li&gt;
&lt;li&gt;ZStack 给VR指定两个特定的IP地址并注入VR VM中。&lt;/li&gt;
&lt;li&gt;VR 启动的最后会调用一个初始化的脚本把注入的IP地址设置到eth0和eth1上面&lt;/li&gt;
&lt;li&gt;ZStack管控程序在创建VR之后就会轮询的用ssh尝试连接VR&lt;/li&gt;
&lt;li&gt;如果ssh连接成功，ZStack管控程序就会通过Ansible 安装和部署VR的管控程序&lt;/li&gt;
&lt;li&gt;VR Agent启动后，ZStack管控程序就会通过HTTP post命令给VR，例如设置即将启动的用户VM的IP地址，DNS之类。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;VR启动成功后，ZStack就会创建用户VM。用户VM的eth0通过br_eth1的网桥连接到VR的eth1上，并且可以和未来的用户VM通讯。
用户VM访问公网的时候，通过VR的SNAT服务中转。用户VM需要EIP、Port Forwarding还有Load Balancer等服务的时候，也都是在VR上进行对应的配置。&lt;/p&gt;

&lt;p&gt;用户在使用ZStack的时候，有时候会遇到VR创建失败的情况。经过分析，其中大多数失败原因都是因为网络配置导致的。具体来看，VR启动失败的故障可能有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建VR VM失败：找不到合适的Host。可能是没有处于Connected状态的Host，或者Host上的空闲资源（CPU，内存）不足以启动VR。&lt;/li&gt;
&lt;li&gt;VR 操作系统启动失败：虚拟化软件错误或者硬盘连接错误（NFS网络不稳定）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; ZStack ssh VR 失败：ZStack管控节点无法连接VR。通常原因有：IP地址配置不对；交换机没有允许对应的IP连接；使用了特别的vlan，但是交换机没有设置Truck模式。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;ZStack Ansible部署VR Agent失败：部署VR的时候可能会连接Internet下载VR需要的系统库，但是这步在0.9之后就不需要从互联网上下载系统库了，所以通常不会出错。&lt;/li&gt;
&lt;li&gt;VR Agent 启动失败：可能是使用了不匹配的VR Image。例如ZStack 0.9 需要使用 VR 0.9版本的Image，如果用户没有更新VR Image的话，会导致HTTP 404的错误。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;上述错误中，最常见的错误是3。大家可以对照解决。&lt;/p&gt;

&lt;p&gt;在VR启动后，用户再启动同一Private L3上的VM的时候ZStack通常不会再次创建VR（除非是使用了独立的负载均衡功能）。
如果在用户VM运行的过程中，发生了VR连接错误，它的影响会是什么？我们该如何恢复呢？下表例举了在多计算节点和不同存储类型的情况下如何恢复失联VR的方法。&lt;/p&gt;

&lt;table class=&quot;table table-striped table-bordered&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;VR失效原因&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;VR失效影响&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;恢复VR方法&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;VR VM网络失联，但是virsh list还可以看到VR 是running状态&lt;/td&gt;
    &lt;td&gt;扁平网络：外网通讯中断、无法创建新VM、内网连接不影响；扁平网络：无法创建新VM，网络连接不影响&lt;/td&gt;
    &lt;td&gt;从ZStack UI中的Virtual Router中delete VR VM， 再去Instance的界面重新创建一个用户VM&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;VR VM 所在Host挂掉，之后手动重启Host&lt;/td&gt;
    &lt;td&gt;非扁平网络：外网通讯中断、无法创建新VM；扁平网络：无法创建新VM，网络不影响&lt;/td&gt;
    &lt;td&gt;从ZStack UI中的Virtual Router中再次Start VR VM&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;VR VM 所在Host挂掉，之后Host无法重启，使用网络共享主存储&lt;/td&gt;
    &lt;td&gt;非扁平网络：外网通讯中断、无法创建新VM、内网不影响；扁平网络：无法创建新VM，网络连接不影响&lt;/td&gt;
    &lt;td&gt;使用zstack-cli UpdateVmInstance 把VR VM的状态改成 Stopped，再从ZStack UI中的Virtual Router中再次Start VR VM&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;VR VM 所在Host挂掉，之后Host无法重启，使用本地主存储&lt;/td&gt;
    &lt;td&gt;非扁平网络：外网通讯中断、无法创建新VM；扁平网络：无法创建新VM，网络连接不影响&lt;/td&gt;
    &lt;td&gt;使用zstack-cli UpdateVmInstance 把VR VM的状态改成 Stopped，从ZStack UI中的Virtual Router中delete VR VM，然后再去Instance的界面创建一个新的用户VM&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


&lt;p&gt;未来ZStack的VR还会提供HA的功能，当一个VR失效的时候，会自动启动一个VR接管之前VR的网络服务。
通常虚拟路由已经可以满足大多数用户正常的网络要求，但是如果用户对网络性能有更高的要求，ZStack也可以集成商业虚拟机交换机，甚至是物理交换机的网络服务。
用户可以在创建L3网络的时候，选择不同网络服务的提供方。&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Sep 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn_blog/virtualrouter-introduction.html</link>
        <guid isPermaLink="true">http://zstack.org/cn_blog/virtualrouter-introduction.html</guid>
        
        
        <category>cn_blog</category>
        
      </item>
    
      <item>
        <title>ZStack v0.9 发布</title>
        <description>&lt;p&gt;ZStack 0.9 版本今天发布，欢迎大家测试试用。在这个版本里，ZStack新加了两大重量级的功能：&lt;/p&gt;

&lt;h2&gt;支持分布式存储Ceph&lt;/h2&gt;

&lt;p&gt;从0.9版本开始，ZStack正式支持Ceph作为主存储和备份存储的设备。为了最大化的利用Ceph存储的高级能力，
在一个Zone内，用户需要使用一个Ceph Cluster同时作为主存储和备份存储。这样做的好处是，
避免了用户在创建新的云主机、备份Volume等操作时，主存储和备份存储之间不必要的数据拷贝，
所有的数据操作都是通过COW（copy on write）来实现的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/1.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;p&gt;ZStack和Ceph之间的交互是通过部署在Ceph Mon服务器上的Agent来完成的。用户可以动态的添加或删除Ceph的Mon服务器。&lt;/p&gt;

&lt;h3&gt;Ceph 备份存储&lt;/h3&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Ceph 备份存储只能和Ceph主存储配合使用&lt;/h4&gt;
  
  使用Ceph的一个主要的好处是因为Ceph支持COW，所以在ZStack中，如果使用了Ceph的备份存储，那么主存储也必须是同一个Ceph。
  由于Ceph备份存储和主存储都使用相同的Ceph cluster，所以用户不能把一个Ceph的备份存储挂载到多个ZStack的Zones上。  最好的使用方法是每一个Zone使用独立的Ceph cluster作为同一套主存储和备份存储。
  
  换句话说，当ZStack的Zone仅仅挂载了Ceph作为备份存储的时候，你不能再使用NFS、本地存储以及ISCSI作为该Zone的主存储。
  当一个Zone同时挂载Ceph备份存储和SFTP备份存储的时候，可以混用多种主存储类型。但是需要特别注意的是，
  如果云主机的镜像文件只存在于Ceph备份存储的时候，该云主机只能在Ceph主存储上创建成功。
  假设创建云主机时选择的L3网络所在的Cluster上没有挂载Ceph主存储，但是该云主机的镜像文件只存在在Ceph备份存储
  上，那么创建云主机将会发生找不到合适主存储的失败。
&lt;/div&gt;


&lt;h4&gt;从UI添加Ceph备份存储&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/2.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择类型 &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的 IP/hostname&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的 SSH 用户名&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的 SSH 密码&lt;/li&gt;
&lt;li&gt;点击 &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;重复步骤 3 ~ 5 来添加其他的Ceph Mon服务器&lt;/li&gt;
&lt;li&gt;点击 &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;通过CLI添加Ceph备份存储&lt;/h4&gt;

&lt;p&gt;你可以使用 AddCephBackupStorage 来添加 Ceph 备份存储. 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephBackupStorage name=ceph monUrls=root:密码@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;monUrls&lt;/code&gt; 是一个字符串列表，每一个Mon服务器的信息通过逗号来分割，每一个Mon服务器的信息遵从如下的格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_用户名:ssh_密码@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_用户名&lt;/code&gt;, &lt;code&gt;ssh_密码&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; 是必须的内容，而&lt;code&gt;ssh_port&lt;/code&gt; 和 &lt;code&gt;ceph_mon_port&lt;/code&gt;是选填的。
&lt;code&gt;ceph_mon_port&lt;/code&gt; 是Ceph Mon服务器的端口，默认值为6789. 一个完整的monUrl的例子是：&lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;指定 pool&lt;/h4&gt;
  &lt;code&gt;AddCephBackupStorage&lt;/code&gt; 有一个特别的参数&lt;code&gt;poolName&lt;/code&gt;。通过它，用户可以指定一个存在的Ceph Pool。 
  当用户指定一个pool的名字的时候，ZStack会使用这个pool。当这个pool不存在的时候，ZStack将会报告一个添加失败的错误。
  当用户不指定特别的pool名字的时候，ZStack会自动创建一个新的pool。
  
  你可以利用这个功能预先创建一个合适的pool。
&lt;/div&gt;


&lt;h4&gt;动态添加Mon服务器&lt;/h4&gt;

&lt;p&gt;在添加了Ceph备份存储后，用户还可以给该备份存储添加新的Ceph Mon服务器：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;动态删除Mon服务器&lt;/h4&gt;

&lt;p&gt;你还可以使用 RemoveMonFromCephBackupStorage 来删除一个Ceph Mon服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;monHostnames&lt;/code&gt; 是一个通过逗号分割的字符串列表，里面是Mon的IP地址。&lt;/p&gt;

&lt;h4&gt;查询Ceph备份存储&lt;/h4&gt;

&lt;p&gt;使用QueryCephBackupStorage可以查询所有Ceph备份存储的详情:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/4.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Ceph主存储&lt;/h3&gt;

&lt;p&gt;Ceph主存储既可以和Ceph备份存储协同工作，也可以使用SFTP备份存储。&lt;/p&gt;

&lt;h4&gt;通过UI添加&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/3.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择类型 &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的IP/hostname&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的SSH 用户名&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的SSH 密码&lt;/li&gt;
&lt;li&gt;点击 &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;重复步骤 3 ~ 5 来添加其他的Ceph Mon服务器&lt;/li&gt;
&lt;li&gt;点击 &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;你可以使用 AddCephPrimaryStorage 来添加一个 Ceph 主存储。 例如：:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephPrimaryStorage name=ceph zoneUuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;monUrls&lt;/code&gt; 是一个字符串列表，每一个Mon服务器的信息通过逗号来分割，每一个Mon服务器的信息遵从如下的格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_用户名:ssh_密码@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_用户名&lt;/code&gt;, &lt;code&gt;ssh_密码&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; 是必须的内容，而&lt;code&gt;ssh_port&lt;/code&gt; 和 &lt;code&gt;ceph_mon_port&lt;/code&gt;是选填的。
&lt;code&gt;ceph_mon_port&lt;/code&gt; 是Ceph Mon服务器的端口，默认值为6789. 一个完整的monUrl的例子是：&lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;  &lt;h4&gt;指定 pool&lt;/h4&gt;
  &lt;code&gt;AddCephPrimaryStorage&lt;/code&gt; 有三个个特别的参数&lt;code&gt;imageCachePoolName, rootVolumePoolName, dataVolumePoolName&lt;/code&gt;。
  通过它们，用户可以指定存在的Ceph Pool作为主存储的Pool。
  当用户指定pool的名字的时候，ZStack会使用这个pool。当pool不存在的时候，ZStack将会报告一个添加失败的错误。
  当用户不指定特别的pool名字的时候，ZStack会自动创建三个新的pool。&lt;/p&gt;

&lt;p&gt;  你可以利用这个功能预先创建合适的pool。在指定Pool的时候，你可以只指定其中的一个或者两个，然后由ZStack来创建其余的Pool。
&lt;/div&gt;&lt;/p&gt;

&lt;h4&gt;动态添加Mon服务器&lt;/h4&gt;

&lt;p&gt;在添加了Ceph主存储后，用户还可以给该主存储添加新的Ceph Mon服务器：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;动态删除Mon服务器&lt;/h4&gt;

&lt;p&gt;你还可以使用 RemoveMonFromCephPrimaryStorage 来删除一个Ceph Mon服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephPrimaryStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;查询Ceph主存储&lt;/h4&gt;

&lt;p&gt;你可以使用QueryCephPrimaryStorage来查询所有的Ceph主存储的详细信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/5.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h2&gt;动态负载均衡&lt;/h2&gt;

&lt;p&gt;从0.9开始，ZStack支持全新的动态负载均衡网络服务。详细的负载均衡介绍请访问&lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/lb.html&quot;&gt;负载均衡用户手册&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;安装 0.9&lt;/h2&gt;

&lt;h3&gt;单节点一键安装&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9
cd zstack-0.9
wget http://download.zstack.org/releases/0.9/0.9.0/zstack-install.sh
bash zstack-install.sh -a
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;多节点安装&lt;/h3&gt;

&lt;p&gt;ZStack 0.9 多节点安装的步骤可以参考&lt;a href=&quot;../installation/multi-node.html&quot;&gt;多节点安装手册&lt;/a&gt;。&lt;/p&gt;

&lt;h2&gt;无缝升级&lt;/h2&gt;

&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;请格外留意在文章最后的虚拟路由器的升级指令&lt;/h4&gt;
  
  由于ZStack 0.9 在制裁动态路由功能的时候，添加了新的虚拟路由功能。所以不论是否使用虚拟路由功能，用户都应该升级
  虚拟路由。详细的升级办法，会写在升级这章的最后。
&lt;/div&gt;




&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;备份数据库&lt;/h4&gt;
  
  虽然ZStack升级程序会进行备份，不过在升级数据库前，强烈建议用户手动&lt;b&gt;备份数据库!&lt;/b&gt;
  您可以使用以下的命令来备份当前zstack的数据库，以防之后的误操作：
  
  &lt;pre&gt;&lt;code&gt;mysqldump -u root -proot_password --host mysql_ip --port mysql_port zstack &gt; path_to_db_dump.sql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;


&lt;h3&gt;快速升级&lt;/h3&gt;

&lt;p&gt;如果你仅仅只有一个管理节点，数据库和Dashboard也装在相同的节点上，那么你就可以用下面的方法快速升级：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl stop_node
mkdir -p zstack-0.9
cd zstack-0.9
wget http://download.zstack.org/releases/0.9/0.9.0/zstack-install.sh
wget http://7xi3lj.com1.z0.glb.clouddn.com/releases/0.9/0.9.0/zstack-all-in-one-0.9.0.tgz
bash zstack-install.sh -u -f zstack-all-in-one-0.9.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常情况，你大概只需要等待2分钟，zstack就会帮你升级完成。&lt;/p&gt;

&lt;h3&gt;使用zstack-ctl升级多节点&lt;/h3&gt;

&lt;h4&gt;1. 升级第一个节点&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9
cd zstack-0.9
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;请重新安装zstack-ctl，如果你还在使用zstack v0.6系列的产品&lt;/h4&gt;
  
  wget http://download.zstack.org/releases/0.7/rc2/zstackctl-0.7.tar.gz
  /var/lib/zstack/virtualenv/zstackctl/bin/pip install --ignore-installed zstackctl-0.7.tar.gz
  
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;wget http://7xi3lj.com1.z0.glb.clouddn.com/releases/0.9/0.9.0/zstack.war
zstack-ctl upgrade_management_node --war-file zstack.war
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. 升级数据库&lt;/h4&gt;

&lt;p&gt;请确保你已经成功备份了数据库！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_db
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;如果只有一个管理节点，您可以立刻启动该节点&lt;/h4&gt;
  使用命令&lt;pre&gt;&lt;code&gt;zstack-ctl start_node&lt;/code&gt;&lt;/pre&gt;启动zstack管理节点。如果还有其他管理节点，请继续完成步骤三。
&lt;/div&gt;


&lt;h4&gt;3. 升级其他管理节点&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_management_node --war-file path_to_the_war --host remote_host_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;4. 升级UI&lt;/h4&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;停止UI服务&lt;/h4&gt;
  
  如果还在使用0.6版本，请使用命令：&lt;code&gt;/etc/init.d/zstack-dashboard stop&lt;/code&gt;; 
  对于0.6以后的版本，请使用命令：&lt;code&gt;zstack-ctl stop_ui&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;升级本地UI服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者升级远端UI服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui --host remote_machine_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5. 启动管理节点&lt;/h4&gt;

&lt;p&gt;启动本地管理节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_node`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动远程管理节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_node --host remote_host_ip`
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;所有的Agent都会随着管理节点的启动而自动升级&lt;/h4&gt;
  当管理节点启动后，会重新连接并升级包括计算节点，备份存储，虚拟路由等等一系列的ZStack Agents。
  用户在创建新的云主机之前，需要确保计算节点的状态已经变成Connected
&lt;/div&gt;


&lt;h4&gt;6. 启动UI服务&lt;/h4&gt;

&lt;p&gt;启动本地UI：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_ui`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动远端UI：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_ui --host remote_host_ip`
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;7. 升级虚拟路由&lt;/h4&gt;

&lt;h5&gt;升级正在运行的虚拟路由器(Virtual Router VMs）&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择 虚拟路由器 VM&lt;/li&gt;
&lt;li&gt;点击 &quot;Action&quot;&lt;/li&gt;
&lt;li&gt;点击 &quot;Reconnect&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;待重连之后，虚拟路由器会更新到最新的版本。&lt;/p&gt;

&lt;h5&gt;更新虚拟路由器镜像以及虚拟路由器模板&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;添加新的虚拟路由器镜像 http://7xi3lj.com1.z0.glb.clouddn.com/releases/0.9/0.9.0/zstack-virtualrouter-0.9.0.qcow2:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;通过CLI更新虚拟路由器模板:&lt;/p&gt;

&lt;p&gt; UpdateVirtualRouterOffering uuid=vr_offering_uuid imageUuid=new_image_uuid&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;你将不能在0.9版本的ZStack中成功使用0.8版本的虚拟路由器镜像创建一个虚拟路由器！&lt;/h4&gt;
  如果你不更新现有的虚拟路由器模板和镜像，当你添加一个新的具有虚拟路由器的L3网络，并用其创建一个新的云主机的时候，
  你将会遇到虚拟路由器连接失败，新的云主机无法创建等错误。所以在正常使用0.9版本之前，务必要更新虚拟路由器镜像。
&lt;/div&gt;


&lt;h2&gt;修复的Bugs&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/123&quot;&gt;Delete VIP hang, when using separated VR to do Load Balancer #123&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/122&quot;&gt;New created volume doesn&#39;t exist, if original volume is deleted in ceph #122&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/121&quot;&gt;Parallel VM creation might fail #121&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/119&quot;&gt;ceph mon url doesn&#39;t support password of @ #119&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/118&quot;&gt;cannot use eth0:1 as L2 interface #118&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/117&quot;&gt;ZStack 0.9 doesn&#39;t have Load Balancer if upgrading from 0.8 #117&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/116&quot;&gt;Delete VR pub L3 IP range when VR is starting, will block user vm creation #116&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/115&quot;&gt;VIP can&#39;t be deleted, when load balancer creation failed #115&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/114&quot;&gt;VIP&#39;s IP won&#39;t be cleaned up in VR, when delete VIP for LB #114&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/113&quot;&gt;Add Nic to listener might be failed #113&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/112&quot;&gt;attach volume might fail #112&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/111&quot;&gt;Ceph mon is not cleaned up, when ceph pool is added failed #111&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/110&quot;&gt;GetFreeIp won&#39;t return next available ip, if previous ip is used #110&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/109&quot;&gt;Delete Ceph PS failed: FutureCompletion timeout after 1 seconds #109&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/108&quot;&gt;Delete VIP failed, due to used by load balancer #108&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/107&quot;&gt;Update System Tags won&#39;t work if using RebootVM &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/106&quot;&gt;Update systemTags for static IP won&#39;t effect #106&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/104&quot;&gt;Delete Ceph PS won&#39;t cleanup pools #104&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/103&quot;&gt;The same ceph primary storage can&#39;t be added to 2 Zones #103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/101&quot;&gt;new added DNS won&#39;t be added to VR, until call Reconnect VR #101&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/100&quot;&gt;DNS is not removed after Delete L3 DNS #100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/99&quot;&gt;VM migration failed if using ceph ps #99&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/98&quot;&gt;Query API with multiple conditions might return wrong result #98&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/97&quot;&gt;Header sub-project build from source code : bad version number found in aspectj. #97&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/96&quot;&gt;Can NOT acquire the lock again before unlock, GLock is non reentrant #96&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/95&quot;&gt;iso duplicate primary key issue when downloading iso to local storage #95&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/94&quot;&gt;BackupVolumeSnapshot doesn&#39;t support with Ceph PS and BS #94&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/93&quot;&gt;Delete Image won&#39;t cleanup rbd in Ceph BS pool #93&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/92&quot;&gt;vm reboot fail when using ceph bs and ps #92&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/91&quot;&gt;vm creation failed with both ceph ps and ceph bs #91&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/90&quot;&gt;create vm fail with ceph ps and sftp bs #90&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/89&quot;&gt;AddCephPrimaryStorage fail #89&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/88&quot;&gt;UpdateImage with platform=Paravirtualization failed #88&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;报告bug&lt;/h2&gt;

&lt;p&gt;如果你在使用中发现任何问题或者有任何建议，请你到我们的&lt;a href=&quot;https://github.com/zstackorg/zstack/issues&quot;&gt;GitHub&lt;/a&gt;上告诉我们，谢谢！&lt;/p&gt;

&lt;p&gt;Enjoy～
ZStack 开发团队&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Sep 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn_blog/v0.9-cn.html</link>
        <guid isPermaLink="true">http://zstack.org/cn_blog/v0.9-cn.html</guid>
        
        
        <category>cn_blog</category>
        
      </item>
    
      <item>
        <title>Announcing ZStack v0.9</title>
        <description>&lt;p&gt;Hello everyone, I am Frank Zhang, the architect of ZStack. Today I am happy to announce that ZStack v0.9 is released.
In this release, ZStack introduces two new features:&lt;/p&gt;

&lt;h2&gt;Ceph Integration&lt;/h2&gt;

&lt;p&gt;Beginning at this version, ZStack supports Ceph as backup storage and primary storage. To leverage the advantages of Ceph,
users should use a Ceph cluster for both backup storage and primary storage in the same zone; by doing so, there will be
no data copy between backup storage and primary storage when users perform operations like creating VM, creating image,
creating a data volume from an image, all are done through COW(copy on write).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/1.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;p&gt;ZStack interoperates with Ceph by deploying agents on Ceph mon servers. Users can dynamically add/remove a Ceph mon server
into/from ZStack.&lt;/p&gt;

&lt;h3&gt;Ceph Backup Storage&lt;/h3&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Ceph backup storage only works with Ceph primary storage&lt;/h4&gt;
  
  Given the a main advantage of using Ceph is COW, Ceph backup storage is designed to only work with Ceph primary storage.
  Because the Ceph backup storage and primary storage use the same Ceph cluster, users should not attach a Ceph backup storage
  to multiple zones. The best practice is to use a Ceph cluster for both backup storage and primary storage in the same zone.
  
  That is to say, you CANNOT use Ceph backup storage with primary storage of NFS, local storage, ISCSI.
&lt;/div&gt;


&lt;h4&gt;Add through UI&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/2.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the type &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;input IP/hostname of a Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH user name of the Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH password of the Ceph mon server&lt;/li&gt;
&lt;li&gt;click button &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;repeat steps 3 ~ 5 to add other Ceph mon servers&lt;/li&gt;
&lt;li&gt;click button &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;You can use AddCephBackupStorage to add a Ceph backup storage. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephBackupStorage name=ceph monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monUrls&lt;/code&gt; is a list of string containing Ceph mon server information, which is in format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_username:ssh_password@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_username&lt;/code&gt;, &lt;code&gt;ssh_password&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; are mandatory while &lt;code&gt;ssh_port&lt;/code&gt; and &lt;code&gt;ceph_mon_port&lt;/code&gt; are optional. &lt;code&gt;ceph_mon_port&lt;/code&gt; is
the Ceph mon server port which is default to 6789. A full example of monUrl is &lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Specifying the pool&lt;/h4&gt;
  &lt;code&gt;AddCephBackupStorage&lt;/code&gt; receives an optional parameter &lt;code&gt;poolName&lt;/code&gt; which allows you to specify an existing
  Ceph pool for the backup storage. If the parameter is provided, ZStack will use the pool instead of creating a new one;
  if the pool is not existing, an error will be raised and the backup storage will fail to be added. If the parameter is omitted,
  ZStack will automatically create a new pool with the default Ceph pool setting.
  
  You can use this feature to create a well-tuned Ceph pool for the backup storage.
&lt;/div&gt;


&lt;h4&gt;Dynamically add new Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use AddMonToCephBackupStorage to add new Ceph mon servers to a Ceph backup storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Dynamically remove Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use RemoveMonFromCephBackupStorage to remove Ceph mon servers from a Ceph backup storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;Query&lt;/h4&gt;

&lt;p&gt;You can use QueryCephBackupStorage to query Ceph backup storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/4.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Ceph Primary Storage&lt;/h3&gt;

&lt;p&gt;The Ceph primary storage works with both SFTP backup storage and Ceph backup storage.&lt;/p&gt;

&lt;h4&gt;Add through UI&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/3.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the type &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;input IP/hostname of a Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH user name of the Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH password of the Ceph mon server&lt;/li&gt;
&lt;li&gt;click button &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;repeat steps 3 ~ 5 to add other Ceph mon servers&lt;/li&gt;
&lt;li&gt;click button &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;You can use AddCephPrimaryStorage to add a Ceph primary storage. For example::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephPrimaryStorage name=ceph zoneUuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monUrls&lt;/code&gt; is a list of string containing Ceph mon server information, which is in format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_username:ssh_password@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_username&lt;/code&gt;, &lt;code&gt;ssh_password&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; are mandatory while &lt;code&gt;ssh_port&lt;/code&gt; and &lt;code&gt;ceph_mon_port&lt;/code&gt; are optional. &lt;code&gt;ceph_mon_port&lt;/code&gt; is
the Ceph mon server port which is default to 6789. A full example of monUrl is &lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Specifying pools&lt;/h4&gt;
  &lt;code&gt;AddCephPrimaryStorage&lt;/code&gt; receives three optional parameters &lt;code&gt;imageCachePoolName, rootVolumePoolName, dataVolumePoolName&lt;/code&gt;
  all of which allow you to specify existing Ceph pools for the primary storage. If the parameters are provided, ZStack will use the existing pools instead of creating new ones;
  if the pools are not existing, an error will be raised and the primary storage will fail to be added. If the parameters are omitted,
  ZStack will automatically create new pools with the default Ceph pool setting.
  
  You can use this feature to create well-tuned Ceph pools for the primary storage. You can choose to only specify parameters(e.g. rootVolumePoolName) for existing pools
  you want to use, and let ZStack to automatically create the rest.
&lt;/div&gt;


&lt;h4&gt;Dynamically add new Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use AddMonToCephPrimaryStorage to add new Ceph mon servers to a Ceph primary storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Dynamically remove Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use RemoveMonFromCephPrimaryStorage to remove Ceph mon servers from a Ceph primary storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephPrimaryStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;Query&lt;/h4&gt;

&lt;p&gt;You can use QueryCephPrimaryStorage to query Ceph primary storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/5.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Elastic Load Balancer&lt;/h2&gt;

&lt;p&gt;Beginning at 0.9, ZStack supports the elastic load balancer. Details can be found at &lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/lb.html&quot;&gt;User Manual - Elastic Load Balancer&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Seamless Upgrade&lt;/h2&gt;

&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;Pay attention to the virtual router upgrade instructions at the end&lt;/h4&gt;
  
  To support the elastic load balancer, you need to upgrade the virtual router VMs in your current ZStack setup.
  Please do read the virtual router upgrade instructions at the end of the upgrade chapter.
&lt;/div&gt;




&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;Backup Database&lt;/h4&gt;
  
  Before performing any upgrade instructions, please backup the current database. This is very &lt;b&gt;IMPORTANT&lt;/b&gt;!
  Though ZStack will automatically backup the current database during upgrade, we strongly recommend you to manually backup the
  database in case any error happens. You can backup the database following:
  
  &lt;pre&gt;&lt;code&gt;mysqldump -u root -proot_password --host mysql_ip --port mysql_port zstack &gt; path_to_db_dump.sql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;


&lt;h3&gt;Upgrade by quick script&lt;/h3&gt;

&lt;p&gt;If you have only one management node, you can upgrade it by ZStack&#39;s installation script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl stop_node
mkdir -p zstack-0.9
cd zstack-0.9
wget http://download.zstack.org/releases/0.9/0.9.0/zstack-install.sh
wget http://download.zstack.org/releases/0.9/0.9.0/zstack-all-in-one-0.9.0.tgz
bash zstack-install.sh -u -f 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Be patient for a few minutes, the script will upgrade the database, management node, zstack-cli, zstack-ctl and zstack-dashboard.&lt;/p&gt;

&lt;h3&gt;Upgrade by zstack-ctl&lt;/h3&gt;

&lt;h4&gt;1. Upgrade the first management node&lt;/h4&gt;

&lt;p&gt;Perform below instructions on one of your management node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9
cd zstack-0.9
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Install zstack-ctl if you are using 0.6 version&lt;/h4&gt;
  
  wget --no-check-certificate https://download.zstack.org/releases/0.7/rc2/zstackctl-0.7.tar.gz
  /var/lib/zstack/virtualenv/zstackctl/bin/pip install --ignore-installed zstackctl-0.7.tar.gz
  
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;wget http://download.zstack.org/releases/0.9/0.9.0/zstack.war
zstack-ctl upgrade_management_node --war-file zstack.war
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. Upgrade the database&lt;/h4&gt;

&lt;p&gt;Make sure you have backup the current database following instructions on the top of this page. Then perform:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_db
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;You can start the node now if you only have one management node&lt;/h4&gt;
  If you have only one management node, you can run &lt;pre&gt;&lt;code&gt;zstack-ctl start_node&lt;/code&gt;&lt;/pre&gt; to start the ZStack now. If you have
  other management nodes to upgrade, continue to perform following instructions.
&lt;/div&gt;


&lt;h4&gt;3. Upgrade other management nodes&lt;/h4&gt;

&lt;p&gt;If you have management nodes running on remote machines, run below commands for each node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_management_node --war-file path_to_the_war --host remote_host_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;4. Upgrade UI&lt;/h4&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Stop UI&lt;/h4&gt;
  
  If you are using 0.6, stop the UI by &lt;code&gt;/etc/init.d/zstack-dashboard stop&lt;/code&gt;; for 0.7 and 0.8, stop the UI by &lt;code&gt;zstack-ctl stop_ui&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;Upgrade your UI on local machine by:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui --host remote_machine_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if the UI is installed on a remote machine.&lt;/p&gt;

&lt;h4&gt;5. Start management nodes&lt;/h4&gt;

&lt;p&gt;Now all your management nodes have been successfully upgraded to the 0.9. You can start them by &lt;code&gt;zstack-ctl start_node&lt;/code&gt; and
&lt;code&gt;zstack-ctl start_node --host remote_host_ip&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Agents will be automatically upgraded after starting management nodes&lt;/h4&gt;
  You don&#39;t need to bother with agent upgrade; they will be upgraded after management nodes reconnect them.
&lt;/div&gt;


&lt;h4&gt;6. Start UI&lt;/h4&gt;

&lt;p&gt;Now you can start the UI by &lt;code&gt;zstack-ctl start_ui&lt;/code&gt; on the local host or &lt;code&gt;zstack-ctl start_ui --host remote_host_ip&lt;/code&gt; on the remote host.&lt;/p&gt;

&lt;h4&gt;7. Upgrade Virtual Router&lt;/h4&gt;

&lt;h5&gt;Upgrade running virtual router VMs&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the virtual router VM&lt;/li&gt;
&lt;li&gt;click button &quot;Action&quot;&lt;/li&gt;
&lt;li&gt;click &quot;Reconnect&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;After reconnecting, the virtual router VM will be upgraded to the latest version.&lt;/p&gt;

&lt;h5&gt;Upgrade virtual router image and virtual router offering&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;add the 0.9 virtual router image http://download.zstack.org/releases/0.9/0.9.0/zstack-virtualrouter-0.9.0.qcow2 to the backup storage:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;update the existing virtual router offering to the new virtual router image by CLI&lt;/p&gt;

&lt;p&gt; UpdateVirtualRouterOffering uuid=vr_offering_uuid imageUuid=new_image_uuid&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;You cannot create new virtual router VMs if you don&#39;t upgrade the virtual offerings&lt;/h4&gt;
  If you don&#39;t upgrade existing virtual router offerings, new virtual router VMs will fail to be created because the old
  0.8(or 0.7, 0.6) image doesn&#39;t contain the agent required by the ZStack 0.9. You can use above CLI command to update existing
  offerings, or simply delete them and create new ones.
&lt;/div&gt;


&lt;h2&gt;Fixed Issues&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/123&quot;&gt;Delete VIP hang, when using separated VR to do Load Balancer #123&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/122&quot;&gt;New created volume doesn&#39;t exist, if original volume is deleted in ceph #122&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/121&quot;&gt;Parallel VM creation might fail #121&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/119&quot;&gt;ceph mon url doesn&#39;t support password of @ #119&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/118&quot;&gt;cannot use eth0:1 as L2 interface #118&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/117&quot;&gt;ZStack 0.9 doesn&#39;t have Load Balancer if upgrading from 0.8 #117&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/116&quot;&gt;Delete VR pub L3 IP range when VR is starting, will block user vm creation #116&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/115&quot;&gt;VIP can&#39;t be deleted, when load balancer creation failed #115&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/114&quot;&gt;VIP&#39;s IP won&#39;t be cleaned up in VR, when delete VIP for LB #114&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/113&quot;&gt;Add Nic to listener might be failed #113&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/112&quot;&gt;attach volume might fail #112&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/111&quot;&gt;Ceph mon is not cleaned up, when ceph pool is added failed #111&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/110&quot;&gt;GetFreeIp won&#39;t return next available ip, if previous ip is used #110&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/109&quot;&gt;Delete Ceph PS failed: FutureCompletion timeout after 1 seconds #109&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/108&quot;&gt;Delete VIP failed, due to used by load balancer #108&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/107&quot;&gt;Update System Tags won&#39;t work if using RebootVM &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/106&quot;&gt;Update systemTags for static IP won&#39;t effect #106&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/104&quot;&gt;Delete Ceph PS won&#39;t cleanup pools #104&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/103&quot;&gt;The same ceph primary storage can&#39;t be added to 2 Zones #103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/101&quot;&gt;new added DNS won&#39;t be added to VR, until call Reconnect VR #101&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/100&quot;&gt;DNS is not removed after Delete L3 DNS #100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/99&quot;&gt;VM migration failed if using ceph ps #99&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/98&quot;&gt;Query API with multiple conditions might return wrong result #98&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/97&quot;&gt;Header sub-project build from source code : bad version number found in aspectj. #97&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/96&quot;&gt;Can NOT acquire the lock again before unlock, GLock is non reentrant #96&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/95&quot;&gt;iso duplicate primary key issue when downloading iso to local storage #95&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/94&quot;&gt;BackupVolumeSnapshot doesn&#39;t support with Ceph PS and BS #94&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/93&quot;&gt;Delete Image won&#39;t cleanup rbd in Ceph BS pool #93&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/92&quot;&gt;vm reboot fail when using ceph bs and ps #92&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/91&quot;&gt;vm creation failed with both ceph ps and ceph bs #91&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/90&quot;&gt;create vm fail with ceph ps and sftp bs #90&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/89&quot;&gt;AddCephPrimaryStorage fail #89&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/88&quot;&gt;UpdateImage with platform=Paravirtualization failed #88&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Bug Report&lt;/h2&gt;

&lt;p&gt;If you find any bugs, please open a ticket on &lt;a href=&quot;https://github.com/zstackorg/zstack/issues&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Sep 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/blog/v0.9.html</link>
        <guid isPermaLink="true">http://zstack.org/blog/v0.9.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Announcing ZStack v0.9</title>
        <description>&lt;p&gt;Hello everyone, I am Frank Zhang, the architect of ZStack. Today I am happy to announce that ZStack v0.9 is released.
In this release, ZStack introduces two new features:&lt;/p&gt;

&lt;h2&gt;Ceph Integration&lt;/h2&gt;

&lt;p&gt;Beginning at this version, ZStack supports Ceph as backup storage and primary storage. To leverage the advantages of Ceph,
users should use a Ceph cluster for both backup storage and primary storage in the same zone; by doing so, there will be
no data copy between backup storage and primary storage when users perform operations like creating VM, creating image,
creating a data volume from an image, all are done through COW(copy on write).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/1.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;p&gt;ZStack interoperates with Ceph by deploying agents on Ceph mon servers. Users can dynamically add/remove a Ceph mon server
into/from ZStack.&lt;/p&gt;

&lt;h3&gt;Ceph Backup Storage&lt;/h3&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Ceph backup storage only works with Ceph primary storage&lt;/h4&gt;
  
  Given the a main advantage of using Ceph is COW, Ceph backup storage is designed to only work with Ceph primary storage.
  Because the Ceph backup storage and primary storage use the same Ceph cluster, users should not attach a Ceph backup storage
  to multiple zones. The best practice is to use a Ceph cluster for both backup storage and primary storage in the same zone.
  
  That is to say, you CANNOT use Ceph backup storage with primary storage of NFS, local storage, ISCSI.
&lt;/div&gt;


&lt;h4&gt;Add through UI&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/2.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the type &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;input IP/hostname of a Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH user name of the Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH password of the Ceph mon server&lt;/li&gt;
&lt;li&gt;click button &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;repeat steps 3 ~ 5 to add other Ceph mon servers&lt;/li&gt;
&lt;li&gt;click button &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;You can use AddCephBackupStorage to add a Ceph backup storage. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephBackupStorage name=ceph monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monUrls&lt;/code&gt; is a list of string containing Ceph mon server information, which is in format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_username:ssh_password@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_username&lt;/code&gt;, &lt;code&gt;ssh_password&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; are mandatory while &lt;code&gt;ssh_port&lt;/code&gt; and &lt;code&gt;ceph_mon_port&lt;/code&gt; are optional. &lt;code&gt;ceph_mon_port&lt;/code&gt; is
the Ceph mon server port which is default to 6789. A full example of monUrl is &lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Specifying the pool&lt;/h4&gt;
  &lt;code&gt;AddCephBackupStorage&lt;/code&gt; receives an optional parameter &lt;code&gt;poolName&lt;/code&gt; which allows you to specify an existing
  Ceph pool for the backup storage. If the parameter is provided, ZStack will use the pool instead of creating a new one;
  if the pool is not existing, an error will be raised and the backup storage will fail to be added. If the parameter is omitted,
  ZStack will automatically create a new pool with the default Ceph pool setting.
  
  You can use this feature to create a well-tuned Ceph pool for the backup storage.
&lt;/div&gt;


&lt;h4&gt;Dynamically add new Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use AddMonToCephBackupStorage to add new Ceph mon servers to a Ceph backup storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Dynamically remove Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use RemoveMonFromCephBackupStorage to remove Ceph mon servers from a Ceph backup storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;Query&lt;/h4&gt;

&lt;p&gt;You can use QueryCephBackupStorage to query Ceph backup storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/4.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Ceph Primary Storage&lt;/h3&gt;

&lt;p&gt;The Ceph primary storage works with both SFTP backup storage and Ceph backup storage.&lt;/p&gt;

&lt;h4&gt;Add through UI&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/3.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the type &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;input IP/hostname of a Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH user name of the Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH password of the Ceph mon server&lt;/li&gt;
&lt;li&gt;click button &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;repeat steps 3 ~ 5 to add other Ceph mon servers&lt;/li&gt;
&lt;li&gt;click button &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;You can use AddCephPrimaryStorage to add a Ceph primary storage. For example::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephPrimaryStorage name=ceph zoneUuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monUrls&lt;/code&gt; is a list of string containing Ceph mon server information, which is in format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_username:ssh_password@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_username&lt;/code&gt;, &lt;code&gt;ssh_password&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; are mandatory while &lt;code&gt;ssh_port&lt;/code&gt; and &lt;code&gt;ceph_mon_port&lt;/code&gt; are optional. &lt;code&gt;ceph_mon_port&lt;/code&gt; is
the Ceph mon server port which is default to 6789. A full example of monUrl is &lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Specifying pools&lt;/h4&gt;
  &lt;code&gt;AddCephPrimaryStorage&lt;/code&gt; receives three optional parameters &lt;code&gt;imageCachePoolName, rootVolumePoolName, dataVolumePoolName&lt;/code&gt;
  all of which allow you to specify existing Ceph pools for the primary storage. If the parameters are provided, ZStack will use the existing pools instead of creating new ones;
  if the pools are not existing, an error will be raised and the primary storage will fail to be added. If the parameters are omitted,
  ZStack will automatically create new pools with the default Ceph pool setting.
  
  You can use this feature to create well-tuned Ceph pools for the primary storage. You can choose to only specify parameters(e.g. rootVolumePoolName) for existing pools
  you want to use, and let ZStack to automatically create the rest.
&lt;/div&gt;


&lt;h4&gt;Dynamically add new Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use AddMonToCephPrimaryStorage to add new Ceph mon servers to a Ceph primary storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Dynamically remove Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use RemoveMonFromCephPrimaryStorage to remove Ceph mon servers from a Ceph primary storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephPrimaryStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;Query&lt;/h4&gt;

&lt;p&gt;You can use QueryCephPrimaryStorage to query Ceph primary storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/5.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Elastic Load Balancer&lt;/h2&gt;

&lt;p&gt;Beginning at 0.9, ZStack supports the elastic load balancer. Details can be found at &lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/lb.html&quot;&gt;User Manual - Elastic Load Balancer&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Seamless Upgrade&lt;/h2&gt;

&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;Pay attention to the virtual router upgrade instructions at the end&lt;/h4&gt;
  
  To support the elastic load balancer, you need to upgrade the virtual router VMs in your current ZStack setup.
  Please do read the virtual router upgrade instructions at the end of the upgrade chapter.
&lt;/div&gt;




&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;Backup Database&lt;/h4&gt;
  
  Before performing any upgrade instructions, please backup the current database. This is very &lt;b&gt;IMPORTANT&lt;/b&gt;!
  Though ZStack will automatically backup the current database during upgrade, we strongly recommend you to manually backup the
  database in case any error happens. You can backup the database following:
  
  &lt;pre&gt;&lt;code&gt;mysqldump -u root -proot_password --host mysql_ip --port mysql_port zstack &gt; path_to_db_dump.sql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;


&lt;h3&gt;Upgrade by quick script&lt;/h3&gt;

&lt;p&gt;If you have only one management node, you can upgrade it by ZStack&#39;s installation script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl stop_node
mkdir -p zstack-0.9
cd zstack-0.9
wget http://download.zstack.org/releases/0.9/0.9.0/zstack-install.sh
wget http://download.zstack.org/releases/0.9/0.9.0/zstack-all-in-one-0.9.0.tgz
bash zstack-install.sh -u -f 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Be patient for a few minutes, the script will upgrade the database, management node, zstack-cli, zstack-ctl and zstack-dashboard.&lt;/p&gt;

&lt;h3&gt;Upgrade by zstack-ctl&lt;/h3&gt;

&lt;h4&gt;1. Upgrade the first management node&lt;/h4&gt;

&lt;p&gt;Perform below instructions on one of your management node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9
cd zstack-0.9
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Install zstack-ctl if you are using 0.6 version&lt;/h4&gt;
  
  wget --no-check-certificate https://download.zstack.org/releases/0.7/rc2/zstackctl-0.7.tar.gz
  /var/lib/zstack/virtualenv/zstackctl/bin/pip install --ignore-installed zstackctl-0.7.tar.gz
  
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;wget http://download.zstack.org/releases/0.9/0.9.0/zstack.war
zstack-ctl upgrade_management_node --war-file zstack.war
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. Upgrade the database&lt;/h4&gt;

&lt;p&gt;Make sure you have backup the current database following instructions on the top of this page. Then perform:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_db
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;You can start the node now if you only have one management node&lt;/h4&gt;
  If you have only one management node, you can run &lt;pre&gt;&lt;code&gt;zstack-ctl start_node&lt;/code&gt;&lt;/pre&gt; to start the ZStack now. If you have
  other management nodes to upgrade, continue to perform following instructions.
&lt;/div&gt;


&lt;h4&gt;3. Upgrade other management nodes&lt;/h4&gt;

&lt;p&gt;If you have management nodes running on remote machines, run below commands for each node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_management_node --war-file path_to_the_war --host remote_host_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;4. Upgrade UI&lt;/h4&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Stop UI&lt;/h4&gt;
  
  If you are using 0.6, stop the UI by &lt;code&gt;/etc/init.d/zstack-dashboard stop&lt;/code&gt;; for 0.7 and 0.8, stop the UI by &lt;code&gt;zstack-ctl stop_ui&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;Upgrade your UI on local machine by:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui --host remote_machine_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if the UI is installed on a remote machine.&lt;/p&gt;

&lt;h4&gt;5. Start management nodes&lt;/h4&gt;

&lt;p&gt;Now all your management nodes have been successfully upgraded to the 0.9. You can start them by &lt;code&gt;zstack-ctl start_node&lt;/code&gt; and
&lt;code&gt;zstack-ctl start_node --host remote_host_ip&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Agents will be automatically upgraded after starting management nodes&lt;/h4&gt;
  You don&#39;t need to bother with agent upgrade; they will be upgraded after management nodes reconnect them.
&lt;/div&gt;


&lt;h4&gt;6. Start UI&lt;/h4&gt;

&lt;p&gt;Now you can start the UI by &lt;code&gt;zstack-ctl start_ui&lt;/code&gt; on the local host or &lt;code&gt;zstack-ctl start_ui --host remote_host_ip&lt;/code&gt; on the remote host.&lt;/p&gt;

&lt;h4&gt;7. Upgrade Virtual Router&lt;/h4&gt;

&lt;h5&gt;Upgrade running virtual router VMs&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the virtual router VM&lt;/li&gt;
&lt;li&gt;click button &quot;Action&quot;&lt;/li&gt;
&lt;li&gt;click &quot;Reconnect&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;After reconnecting, the virtual router VM will be upgraded to the latest version.&lt;/p&gt;

&lt;h5&gt;Upgrade virtual router image and virtual router offering&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;add the 0.9 virtual router image http://download.zstack.org/releases/0.9/0.9.0/zstack-virtualrouter-0.9.0.qcow2 to the backup storage:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;update the existing virtual router offering to the new virtual router image by CLI&lt;/p&gt;

&lt;p&gt; UpdateVirtualRouterOffering uuid=vr_offering_uuid imageUuid=new_image_uuid&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;You cannot create new virtual router VMs if you don&#39;t upgrade the virtual offerings&lt;/h4&gt;
  If you don&#39;t upgrade existing virtual router offerings, new virtual router VMs will fail to be created because the old
  0.8(or 0.7, 0.6) image doesn&#39;t contain the agent required by the ZStack 0.9. You can use above CLI command to update existing
  offerings, or simply delete them and create new ones.
&lt;/div&gt;


&lt;h2&gt;Fixed Issues&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/123&quot;&gt;Delete VIP hang, when using separated VR to do Load Balancer #123&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/122&quot;&gt;New created volume doesn&#39;t exist, if original volume is deleted in ceph #122&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/121&quot;&gt;Parallel VM creation might fail #121&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/119&quot;&gt;ceph mon url doesn&#39;t support password of @ #119&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/118&quot;&gt;cannot use eth0:1 as L2 interface #118&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/117&quot;&gt;ZStack 0.9 doesn&#39;t have Load Balancer if upgrading from 0.8 #117&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/116&quot;&gt;Delete VR pub L3 IP range when VR is starting, will block user vm creation #116&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/115&quot;&gt;VIP can&#39;t be deleted, when load balancer creation failed #115&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/114&quot;&gt;VIP&#39;s IP won&#39;t be cleaned up in VR, when delete VIP for LB #114&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/113&quot;&gt;Add Nic to listener might be failed #113&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/112&quot;&gt;attach volume might fail #112&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/111&quot;&gt;Ceph mon is not cleaned up, when ceph pool is added failed #111&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/110&quot;&gt;GetFreeIp won&#39;t return next available ip, if previous ip is used #110&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/109&quot;&gt;Delete Ceph PS failed: FutureCompletion timeout after 1 seconds #109&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/108&quot;&gt;Delete VIP failed, due to used by load balancer #108&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/107&quot;&gt;Update System Tags won&#39;t work if using RebootVM &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/106&quot;&gt;Update systemTags for static IP won&#39;t effect #106&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/104&quot;&gt;Delete Ceph PS won&#39;t cleanup pools #104&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/103&quot;&gt;The same ceph primary storage can&#39;t be added to 2 Zones #103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/101&quot;&gt;new added DNS won&#39;t be added to VR, until call Reconnect VR #101&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/100&quot;&gt;DNS is not removed after Delete L3 DNS #100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/99&quot;&gt;VM migration failed if using ceph ps #99&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/98&quot;&gt;Query API with multiple conditions might return wrong result #98&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/97&quot;&gt;Header sub-project build from source code : bad version number found in aspectj. #97&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/96&quot;&gt;Can NOT acquire the lock again before unlock, GLock is non reentrant #96&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/95&quot;&gt;iso duplicate primary key issue when downloading iso to local storage #95&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/94&quot;&gt;BackupVolumeSnapshot doesn&#39;t support with Ceph PS and BS #94&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/93&quot;&gt;Delete Image won&#39;t cleanup rbd in Ceph BS pool #93&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/92&quot;&gt;vm reboot fail when using ceph bs and ps #92&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/91&quot;&gt;vm creation failed with both ceph ps and ceph bs #91&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/90&quot;&gt;create vm fail with ceph ps and sftp bs #90&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/89&quot;&gt;AddCephPrimaryStorage fail #89&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/88&quot;&gt;UpdateImage with platform=Paravirtualization failed #88&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Bug Report&lt;/h2&gt;

&lt;p&gt;If you find any bugs, please open a ticket on &lt;a href=&quot;https://github.com/zstackorg/zstack/issues&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Sep 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn/blog/v0.9.html</link>
        <guid isPermaLink="true">http://zstack.org/cn/blog/v0.9.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>ZStack登录台湾——第一个繁体中文网站 zstack.tw 上线</title>
        <description>&lt;p&gt;近日，来自台湾的&lt;strong&gt;百原科技公司&lt;/strong&gt;推出了第一个&lt;strong&gt;ZStack繁体中文网站&lt;/strong&gt; &lt;code&gt;http://zstack.tw&lt;/code&gt; 。
该网站是继ZStack社区官网之后，世界上第二个以ZStack为主题的网站。
目前虽然该网站的主要内容来自于ZStack官网，但是通过百原工程师对网站内容进行繁体中文的加工，
辅以修改了部分IT用词之后，使得该网站更符合台、港、澳以及部分海外地区中文用户的阅读使用习惯，大大方便他们学习和使用ZStack。&lt;/p&gt;

&lt;p&gt;&lt;img  class=&quot;img-responsive&quot;  src=&quot;/images/blogs/zstack_tw/zstack_tw.png&quot;&gt;&lt;/p&gt;

&lt;hr&gt;


&lt;p&gt;台湾百原科技是一家网络科技公司。他们在服务器虚拟化、
云计算解决方案（ZStack、seafile、Docker、KVM、VMWare、Xen、Hyper-V）、SaaS（Citrix XenApp），
分布式存储（GlusterFS、Ceph、Rockstor）等领域有深厚的实战经验。
他们客户的需求主要覆盖了信息安全，网络购物、以及云端商城建设。
百原科技也曾使用过OpenStack和CloudStack的解决方案，不过ZStack的方便快捷的能力更加合适该公司的发展方向。
基于以往的实战经验，他们快速的构建了ZStack解决方案，并开始在台湾地区推广ZStack项目。&lt;/p&gt;

&lt;p&gt;ZStack社区期待ZStack可以帮助台港澳地区云计算的发展和普及，
并期望ZStack的社区活动&lt;strong&gt;台湾站&lt;/strong&gt;能够在今年晚些时候顺利成行。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;相关链接：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ZStack台灣http://www.zstack.tw
 百原公司官网：http://www.baiyuan.com.tw
 联系方式info@baiyuan.com.tw
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Tue, 01 Sep 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn_blog/zstack-tw-online.html</link>
        <guid isPermaLink="true">http://zstack.org/cn_blog/zstack-tw-online.html</guid>
        
        
        <category>cn_blog</category>
        
      </item>
    
      <item>
        <title>ZStack v0.9 RC2 发布</title>
        <description>&lt;p&gt;ZStack 0.9 RC2版本今天发布，欢迎大家测试试用。在这个版本里，ZStack新加了两大重量级的功能：&lt;/p&gt;

&lt;h2&gt;支持分布式存储Ceph&lt;/h2&gt;

&lt;p&gt;从0.9版本开始，ZStack正式支持Ceph作为主存储和备份存储的设备。为了最大化的利用Ceph存储的高级能力，
在一个Zone内，用户需要使用一个Ceph Cluster同时作为主存储和备份存储。这样做的好处是，
避免了用户在创建新的云主机、备份Volume等操作时，主存储和备份存储之间不必要的数据拷贝，
所有的数据操作都是通过COW（copy on write）来实现的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/1.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;p&gt;ZStack和Ceph之间的交互是通过部署在Ceph Mon服务器上的Agent来完成的。用户可以动态的添加或删除Ceph的Mon服务器。&lt;/p&gt;

&lt;h3&gt;Ceph 备份存储&lt;/h3&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Ceph 备份存储只能和Ceph主存储配合使用&lt;/h4&gt;
  
  使用Ceph的一个主要的好处是因为Ceph支持COW，所以在ZStack中，如果使用了Ceph的备份存储，那么主存储也必须是同一个Ceph。
  由于Ceph备份存储和主存储都使用相同的Ceph cluster，所以用户不能把一个Ceph的备份存储挂载到多个ZStack的Zones上。  最好的使用方法是每一个Zone使用独立的Ceph cluster作为同一套主存储和备份存储。
  
  换句话说，当ZStack的Zone仅仅挂载了Ceph作为备份存储的时候，你不能再使用NFS、本地存储以及ISCSI作为该Zone的主存储。
  当一个Zone同时挂载Ceph备份存储和SFTP备份存储的时候，可以混用多种主存储类型。但是需要特别注意的是，
  如果云主机的镜像文件只存在于Ceph备份存储的时候，该云主机只能在Ceph主存储上创建成功。
  假设创建云主机时选择的L3网络所在的Cluster上没有挂载Ceph主存储，但是该云主机的镜像文件只存在在Ceph备份存储
  上，那么创建云主机将会发生找不到合适主存储的失败。
&lt;/div&gt;


&lt;h4&gt;从UI添加Ceph备份存储&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/2.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择类型 &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的 IP/hostname&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的 SSH 用户名&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的 SSH 密码&lt;/li&gt;
&lt;li&gt;点击 &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;重复步骤 3 ~ 5 来添加其他的Ceph Mon服务器&lt;/li&gt;
&lt;li&gt;点击 &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;通过CLI添加Ceph备份存储&lt;/h4&gt;

&lt;p&gt;你可以使用 AddCephBackupStorage 来添加 Ceph 备份存储. 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephBackupStorage name=ceph monUrls=root:密码@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;monUrls&lt;/code&gt; 是一个字符串列表，每一个Mon服务器的信息通过逗号来分割，每一个Mon服务器的信息遵从如下的格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_用户名:ssh_密码@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_用户名&lt;/code&gt;, &lt;code&gt;ssh_密码&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; 是必须的内容，而&lt;code&gt;ssh_port&lt;/code&gt; 和 &lt;code&gt;ceph_mon_port&lt;/code&gt;是选填的。
&lt;code&gt;ceph_mon_port&lt;/code&gt; 是Ceph Mon服务器的端口，默认值为6789. 一个完整的monUrl的例子是：&lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;指定 pool&lt;/h4&gt;
  &lt;code&gt;AddCephBackupStorage&lt;/code&gt; 有一个特别的参数&lt;code&gt;poolName&lt;/code&gt;。通过它，用户可以指定一个存在的Ceph Pool。 
  当用户指定一个pool的名字的时候，ZStack会使用这个pool。当这个pool不存在的时候，ZStack将会报告一个添加失败的错误。
  当用户不指定特别的pool名字的时候，ZStack会自动创建一个新的pool。
  
  你可以利用这个功能预先创建一个合适的pool。
&lt;/div&gt;


&lt;h4&gt;动态添加Mon服务器&lt;/h4&gt;

&lt;p&gt;在添加了Ceph备份存储后，用户还可以给该备份存储添加新的Ceph Mon服务器：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;动态删除Mon服务器&lt;/h4&gt;

&lt;p&gt;你还可以使用 RemoveMonFromCephBackupStorage 来删除一个Ceph Mon服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;monHostnames&lt;/code&gt; 是一个通过逗号分割的字符串列表，里面是Mon的IP地址。&lt;/p&gt;

&lt;h4&gt;查询Ceph备份存储&lt;/h4&gt;

&lt;p&gt;使用QueryCephBackupStorage可以查询所有Ceph备份存储的详情:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/4.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Ceph主存储&lt;/h3&gt;

&lt;p&gt;Ceph主存储既可以和Ceph备份存储协同工作，也可以使用SFTP备份存储。&lt;/p&gt;

&lt;h4&gt;通过UI添加&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/3.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择类型 &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的IP/hostname&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的SSH 用户名&lt;/li&gt;
&lt;li&gt;输入Ceph mon server的SSH 密码&lt;/li&gt;
&lt;li&gt;点击 &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;重复步骤 3 ~ 5 来添加其他的Ceph Mon服务器&lt;/li&gt;
&lt;li&gt;点击 &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;你可以使用 AddCephPrimaryStorage 来添加一个 Ceph 主存储。 例如：:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephPrimaryStorage name=ceph zoneUuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;monUrls&lt;/code&gt; 是一个字符串列表，每一个Mon服务器的信息通过逗号来分割，每一个Mon服务器的信息遵从如下的格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_用户名:ssh_密码@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_用户名&lt;/code&gt;, &lt;code&gt;ssh_密码&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; 是必须的内容，而&lt;code&gt;ssh_port&lt;/code&gt; 和 &lt;code&gt;ceph_mon_port&lt;/code&gt;是选填的。
&lt;code&gt;ceph_mon_port&lt;/code&gt; 是Ceph Mon服务器的端口，默认值为6789. 一个完整的monUrl的例子是：&lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;  &lt;h4&gt;指定 pool&lt;/h4&gt;
  &lt;code&gt;AddCephPrimaryStorage&lt;/code&gt; 有三个个特别的参数&lt;code&gt;imageCachePoolName, rootVolumePoolName, dataVolumePoolName&lt;/code&gt;。
  通过它们，用户可以指定存在的Ceph Pool作为主存储的Pool。
  当用户指定pool的名字的时候，ZStack会使用这个pool。当pool不存在的时候，ZStack将会报告一个添加失败的错误。
  当用户不指定特别的pool名字的时候，ZStack会自动创建三个新的pool。&lt;/p&gt;

&lt;p&gt;  你可以利用这个功能预先创建合适的pool。在指定Pool的时候，你可以只指定其中的一个或者两个，然后由ZStack来创建其余的Pool。
&lt;/div&gt;&lt;/p&gt;

&lt;h4&gt;动态添加Mon服务器&lt;/h4&gt;

&lt;p&gt;在添加了Ceph主存储后，用户还可以给该主存储添加新的Ceph Mon服务器：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;动态删除Mon服务器&lt;/h4&gt;

&lt;p&gt;你还可以使用 RemoveMonFromCephPrimaryStorage 来删除一个Ceph Mon服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephPrimaryStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;查询Ceph主存储&lt;/h4&gt;

&lt;p&gt;你可以使用QueryCephPrimaryStorage来查询所有的Ceph主存储的详细信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/5.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h2&gt;动态负载均衡&lt;/h2&gt;

&lt;p&gt;从0.9开始，ZStack支持全新的动态负载均衡网络服务。详细的负载均衡介绍请访问&lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/lb.html&quot;&gt;负载均衡用户手册&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;需要说明的是，RC2版本中有一个&lt;a href=&quot;https://github.com/zstackorg/zstack/issues/117&quot;&gt;Bug&lt;/a&gt;，导致通过无缝升级的ZStack无法使用负载均衡的功能。
用户暂时需要重新安装ZStack才可以使用。我们将在正式版中修复这个Bug。&lt;/p&gt;

&lt;h2&gt;安装 0.9 RC2&lt;/h2&gt;

&lt;h3&gt;单节点一键安装&lt;/h3&gt;

&lt;p&gt;目前官网的安装脚本只会安装0.8正式版的ZStack。安装0.9 RC2版本的ZStack需要单独下载安装包，并通过安装脚本指定安装包路径：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9-rc2
cd zstack-0.9-rc2
wget http://download.zstack.org/releases/0.9/rc2/zstack-install.sh
wget http://download.zstack.org/releases/0.9/rc2/zstack-all-in-one-0.9.0-rc2.tgz
bash zstack-install.sh -a -f zstack-all-in-one-0.9.0-rc2.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;多节点安装&lt;/h3&gt;

&lt;p&gt;ZStack 0.9 RC2多节点安装的步骤可以参考&lt;a href=&quot;../installation/multi-node.html&quot;&gt;多节点安装手册&lt;/a&gt;。注意的是，需要提前下载0.9 RC2的安装包，并在安装的时候通过 &lt;code&gt;-f zstack-all-in-one-0.9.0-rc2.tgz&lt;/code&gt; 来指定。&lt;/p&gt;

&lt;h2&gt;无缝升级&lt;/h2&gt;

&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;请格外留意在文章最后的虚拟路由器的升级指令&lt;/h4&gt;
  
  由于ZStack0.9在制裁动态路由功能的时候，添加了新的虚拟路由功能。所以不论是否使用虚拟路由功能，用户都应该升级
  虚拟路由。详细的升级办法，会写在升级这章的最后。
&lt;/div&gt;




&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;备份数据库&lt;/h4&gt;
  
  虽然ZStack升级程序会进行备份，不过在升级数据库前，强烈建议用户手动&lt;b&gt;备份数据库!&lt;/b&gt;
  您可以使用以下的命令来备份当前zstack的数据库，以防之后的误操作：
  
  &lt;pre&gt;&lt;code&gt;mysqldump -u root -proot_password --host mysql_ip --port mysql_port zstack &gt; path_to_db_dump.sql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;


&lt;h3&gt;快速升级&lt;/h3&gt;

&lt;p&gt;如果你仅仅只有一个管理节点，数据库和Dashboard也装在相同的节点上，那么你就可以用下面的方法快速升级：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl stop_node
mkdir -p zstack-0.9-rc2
cd zstack-0.9-rc2
wget http://download.zstack.org/releases/0.9/rc2/zstack-install.sh
wget http://download.zstack.org/releases/0.9/rc2/zstack-all-in-one-0.9.0-rc2.tgz
bash zstack-install.sh -u -f zstack-all-in-one-0.9.0-rc2.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常情况，你大概只需要等待2分钟，zstack就会帮你升级完成。&lt;/p&gt;

&lt;h3&gt;使用zstack-ctl升级多节点&lt;/h3&gt;

&lt;h4&gt;1. 升级第一个节点&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9-rc2
cd zstack-0.9-rc2
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;请重新安装zstack-ctl，如果你还在使用zstack v0.6系列的产品&lt;/h4&gt;
  
  wget http://download.zstack.org/releases/0.7/rc2/zstackctl-0.7.tar.gz
  /var/lib/zstack/virtualenv/zstackctl/bin/pip install --ignore-installed zstackctl-0.7.tar.gz
  
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;wget http://download.zstack.org/releases/0.9/rc2/zstack.war
zstack-ctl upgrade_management_node --war-file zstack.war
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. 升级数据库&lt;/h4&gt;

&lt;p&gt;请确保你已经成功备份了数据库！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_db
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;如果只有一个管理节点，您可以立刻启动该节点&lt;/h4&gt;
  使用命令&lt;pre&gt;&lt;code&gt;zstack-ctl start_node&lt;/code&gt;&lt;/pre&gt;启动zstack管理节点。如果还有其他管理节点，请继续完成步骤三。
&lt;/div&gt;


&lt;h4&gt;3. 升级其他管理节点&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_management_node --war-file path_to_the_war --host remote_host_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;4. 升级UI&lt;/h4&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;停止UI服务&lt;/h4&gt;
  
  如果还在使用0.6版本，请使用命令：&lt;code&gt;/etc/init.d/zstack-dashboard stop&lt;/code&gt;; 
  对于0.6以后的版本，请使用命令：&lt;code&gt;zstack-ctl stop_ui&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;升级本地UI服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者升级远端UI服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui --host remote_machine_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5. 启动管理节点&lt;/h4&gt;

&lt;p&gt;启动本地管理节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_node`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动远程管理节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_node --host remote_host_ip`
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;所有的Agent都会随着管理节点的启动而自动升级&lt;/h4&gt;
  当管理节点启动后，会重新连接并升级包括计算节点，备份存储，虚拟路由等等一系列的ZStack Agents。
  用户在创建新的云主机之前，需要确保计算节点的状态已经变成Connected
&lt;/div&gt;


&lt;h4&gt;6. 启动UI服务&lt;/h4&gt;

&lt;p&gt;启动本地UI：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_ui`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动远端UI：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`zstack-ctl start_ui --host remote_host_ip`
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;7. 升级虚拟路由&lt;/h4&gt;

&lt;h5&gt;升级正在运行的虚拟路由器(Virtual Router VMs）&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择 虚拟路由器 VM&lt;/li&gt;
&lt;li&gt;点击 &quot;Action&quot;&lt;/li&gt;
&lt;li&gt;点击 &quot;Reconnect&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;待重连之后，虚拟路由器会更新到最新的版本。&lt;/p&gt;

&lt;h5&gt;更新虚拟路由器镜像以及虚拟路由器模板&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;添加新的虚拟路由器镜像 http://download.zstack.org/releases/0.9/rc2/zstack-virtualrouter-0.9.0-rc.qcow2:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;通过CLI更新虚拟路由器模板:&lt;/p&gt;

&lt;p&gt; UpdateVirtualRouterOffering uuid=vr_offering_uuid imageUuid=new_image_uuid&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;你将不能在0.9版本的ZStack中成功使用0.8版本的虚拟路由器镜像创建一个虚拟路由器！&lt;/h4&gt;
  如果你不更新现有的虚拟路由器模板和镜像，当你添加一个新的具有虚拟路由器的L3网络，并用其创建一个新的云主机的时候，
  你将会遇到虚拟路由器连接失败，新的云主机无法创建等错误。所以在正常使用0.9版本之前，务必要更新虚拟路由器镜像。
&lt;/div&gt;


&lt;h2&gt;报告bug&lt;/h2&gt;

&lt;p&gt;如果你在使用中发现任何问题或者有任何建议，请你到我们的&lt;a href=&quot;https://github.com/zstackorg/zstack/issues&quot;&gt;GitHub&lt;/a&gt;上告诉我们，谢谢！&lt;/p&gt;

&lt;p&gt;Enjoy～
ZStack 开发团队&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Aug 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn/blog/v0.9-rc2.html</link>
        <guid isPermaLink="true">http://zstack.org/cn/blog/v0.9-rc2.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Announcing ZStack v0.9 RC2</title>
        <description>&lt;p&gt;Hello everyone, I am Frank Zhang, the architect of ZStack. Today I am happy to announce that ZStack v0.9 is in the release cycle.
Today we release 0.9 RC2 for you test. In this release, ZStack introduces two new features:&lt;/p&gt;

&lt;h2&gt;Ceph Integration&lt;/h2&gt;

&lt;p&gt;Beginning at this version, ZStack supports Ceph as backup storage and primary storage. To leverage the advantages of Ceph,
users should use a Ceph cluster for both backup storage and primary storage in the same zone; by doing so, there will be
no data copy between backup storage and primary storage when users perform operations like creating VM, creating image,
creating a data volume from an image, all are done through COW(copy on write).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/1.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;p&gt;ZStack interoperates with Ceph by deploying agents on Ceph mon servers. Users can dynamically add/remove a Ceph mon server
into/from ZStack.&lt;/p&gt;

&lt;h3&gt;Ceph Backup Storage&lt;/h3&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Ceph backup storage only works with Ceph primary storage&lt;/h4&gt;
  
  Given the a main advantage of using Ceph is COW, Ceph backup storage is designed to only work with Ceph primary storage.
  Because the Ceph backup storage and primary storage use the same Ceph cluster, users should not attach a Ceph backup storage
  to multiple zones. The best practice is to use a Ceph cluster for both backup storage and primary storage in the same zone.
  
  That is to say, you CANNOT use Ceph backup storage with primary storage of NFS, local storage, ISCSI.
&lt;/div&gt;


&lt;h4&gt;Add through UI&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/2.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the type &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;input IP/hostname of a Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH user name of the Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH password of the Ceph mon server&lt;/li&gt;
&lt;li&gt;click button &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;repeat steps 3 ~ 5 to add other Ceph mon servers&lt;/li&gt;
&lt;li&gt;click button &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;You can use AddCephBackupStorage to add a Ceph backup storage. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephBackupStorage name=ceph monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monUrls&lt;/code&gt; is a list of string containing Ceph mon server information, which is in format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_username:ssh_password@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_username&lt;/code&gt;, &lt;code&gt;ssh_password&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; are mandatory while &lt;code&gt;ssh_port&lt;/code&gt; and &lt;code&gt;ceph_mon_port&lt;/code&gt; are optional. &lt;code&gt;ceph_mon_port&lt;/code&gt; is
the Ceph mon server port which is default to 6789. A full example of monUrl is &lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Specifying the pool&lt;/h4&gt;
  &lt;code&gt;AddCephBackupStorage&lt;/code&gt; receives an optional parameter &lt;code&gt;poolName&lt;/code&gt; which allows you to specify an existing
  Ceph pool for the backup storage. If the parameter is provided, ZStack will use the pool instead of creating a new one;
  if the pool is not existing, an error will be raised and the backup storage will fail to be added. If the parameter is omitted,
  ZStack will automatically create a new pool with the default Ceph pool setting.
  
  You can use this feature to create a well-tuned Ceph pool for the backup storage.
&lt;/div&gt;


&lt;h4&gt;Dynamically add new Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use AddMonToCephBackupStorage to add new Ceph mon servers to a Ceph backup storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Dynamically remove Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use RemoveMonFromCephBackupStorage to remove Ceph mon servers from a Ceph backup storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;Query&lt;/h4&gt;

&lt;p&gt;You can use QueryCephBackupStorage to query Ceph backup storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/4.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Ceph Primary Storage&lt;/h3&gt;

&lt;p&gt;The Ceph primary storage works with both SFTP backup storage and Ceph backup storage.&lt;/p&gt;

&lt;h4&gt;Add through UI&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/3.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the type &#39;Ceph&#39;&lt;/li&gt;
&lt;li&gt;input IP/hostname of a Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH user name of the Ceph mon server&lt;/li&gt;
&lt;li&gt;input SSH password of the Ceph mon server&lt;/li&gt;
&lt;li&gt;click button &#39;Add&#39;&lt;/li&gt;
&lt;li&gt;repeat steps 3 ~ 5 to add other Ceph mon servers&lt;/li&gt;
&lt;li&gt;click button &#39;Next&#39;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Add through CLI&lt;/h4&gt;

&lt;p&gt;You can use AddCephPrimaryStorage to add a Ceph primary storage. For example::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddCephPrimaryStorage name=ceph zoneUuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monUrls&lt;/code&gt; is a list of string containing Ceph mon server information, which is in format of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_username:ssh_password@mon_server_ip:[ssh_port][/?monPort=ceph_mon_port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ssh_username&lt;/code&gt;, &lt;code&gt;ssh_password&lt;/code&gt;, &lt;code&gt;mon_server_ip&lt;/code&gt; are mandatory while &lt;code&gt;ssh_port&lt;/code&gt; and &lt;code&gt;ceph_mon_port&lt;/code&gt; are optional. &lt;code&gt;ceph_mon_port&lt;/code&gt; is
the Ceph mon server port which is default to 6789. A full example of monUrl is &lt;code&gt;root:password@192.168.0.123:22/?monPort=6789&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Specifying pools&lt;/h4&gt;
  &lt;code&gt;AddCephPrimaryStorage&lt;/code&gt; receives three optional parameters &lt;code&gt;imageCachePoolName, rootVolumePoolName, dataVolumePoolName&lt;/code&gt;
  all of which allow you to specify existing Ceph pools for the primary storage. If the parameters are provided, ZStack will use the existing pools instead of creating new ones;
  if the pools are not existing, an error will be raised and the primary storage will fail to be added. If the parameters are omitted,
  ZStack will automatically create new pools with the default Ceph pool setting.
  
  You can use this feature to create well-tuned Ceph pools for the primary storage. You can choose to only specify parameters(e.g. rootVolumePoolName) for existing pools
  you want to use, and let ZStack to automatically create the rest.
&lt;/div&gt;


&lt;h4&gt;Dynamically add new Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use AddMonToCephPrimaryStorage to add new Ceph mon servers to a Ceph primary storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;AddMonToCephBackupStorage uuid=d914841733fa499c9dc6d63ea339469d monUrls=root:password@192.168.0.123,root:password@192.168.0.124,root:password@192.168.0.125
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Dynamically remove Ceph mon servers&lt;/h4&gt;

&lt;p&gt;You can use RemoveMonFromCephPrimaryStorage to remove Ceph mon servers from a Ceph primary storage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;RemoveMonFromCephPrimaryStorage uuid=d914841733fa499c9dc6d63ea339469d monHostnames=192.168.0.123,192.168.0.124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;monHostnames&lt;/code&gt; is a list of IPs of mon servers that you want to remove.&lt;/p&gt;

&lt;h4&gt;Query&lt;/h4&gt;

&lt;p&gt;You can use QueryCephPrimaryStorage to query Ceph primary storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/5.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Elastic Load Balancer&lt;/h2&gt;

&lt;p&gt;Beginning at 0.9, ZStack supports the elastic load balancer. Details can be found at &lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/lb.html&quot;&gt;User Manual - Elastic Load Balancer&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Seamless Upgrade&lt;/h2&gt;

&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;Pay attention to the virtual router upgrade instructions at the end&lt;/h4&gt;
  
  To support the elastic load balancer, you need to upgrade the virtual router VMs in your current ZStack setup.
  Please do read the virtual router upgrade instructions at the end of the upgrade chapter.
&lt;/div&gt;




&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;Backup Database&lt;/h4&gt;
  
  Before performing any upgrade instructions, please backup the current database. This is very &lt;b&gt;IMPORTANT&lt;/b&gt;!
  Though ZStack will automatically backup the current database during upgrade, we strongly recommend you to manually backup the
  database in case any error happens. You can backup the database following:
  
  &lt;pre&gt;&lt;code&gt;mysqldump -u root -proot_password --host mysql_ip --port mysql_port zstack &gt; path_to_db_dump.sql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;


&lt;h3&gt;Upgrade by quick script&lt;/h3&gt;

&lt;p&gt;If you have only one management node, you can upgrade it by ZStack&#39;s installation script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl stop_node
mkdir -p zstack-0.9-rc2
cd zstack-0.9-rc2
wget http://download.zstack.org/releases/0.9/rc2/zstack-install.sh
wget http://download.zstack.org/releases/0.9/rc2/zstack-all-in-one-0.9.0-rc2.tgz
bash zstack-install.sh -u -f zstack-all-in-one-0.9.0-rc2.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Be patient for a few minutes, the script will upgrade the database, management node, zstack-cli, zstack-ctl and zstack-dashboard.&lt;/p&gt;

&lt;h3&gt;Upgrade by zstack-ctl&lt;/h3&gt;

&lt;h4&gt;1. Upgrade the first management node&lt;/h4&gt;

&lt;p&gt;Perform below instructions on one of your management node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p zstack-0.9-rc2
cd zstack-0.9-rc2
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Install zstack-ctl if you are using 0.6 version&lt;/h4&gt;
  
  wget --no-check-certificate https://download.zstack.org/releases/0.7/rc2/zstackctl-0.7.tar.gz
  /var/lib/zstack/virtualenv/zstackctl/bin/pip install --ignore-installed zstackctl-0.7.tar.gz
  
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;wget http://download.zstack.org/releases/0.9/rc2/zstack.war
zstack-ctl upgrade_management_node --war-file zstack.war
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. Upgrade the database&lt;/h4&gt;

&lt;p&gt;Make sure you have backup the current database following instructions on the top of this page. Then perform:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_db
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;You can start the node now if you only have one management node&lt;/h4&gt;
  If you have only one management node, you can run &lt;pre&gt;&lt;code&gt;zstack-ctl start_node&lt;/code&gt;&lt;/pre&gt; to start the ZStack now. If you have
  other management nodes to upgrade, continue to perform following instructions.
&lt;/div&gt;


&lt;h4&gt;3. Upgrade other management nodes&lt;/h4&gt;

&lt;p&gt;If you have management nodes running on remote machines, run below commands for each node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl upgrade_management_node --war-file path_to_the_war --host remote_host_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;4. Upgrade UI&lt;/h4&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Stop UI&lt;/h4&gt;
  
  If you are using 0.6, stop the UI by &lt;code&gt;/etc/init.d/zstack-dashboard stop&lt;/code&gt;; for 0.7 and 0.8, stop the UI by &lt;code&gt;zstack-ctl stop_ui&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;Upgrade your UI on local machine by:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zstack-ctl install_ui --host remote_machine_ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if the UI is installed on a remote machine.&lt;/p&gt;

&lt;h4&gt;5. Start management nodes&lt;/h4&gt;

&lt;p&gt;Now all your management nodes have been successfully upgraded to the 0.8 RC2. You can start them by &lt;code&gt;zstack-ctl start_node&lt;/code&gt; and
&lt;code&gt;zstack-ctl start_node --host remote_host_ip&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;bs-callout bs-callout-info&quot;&gt;
  &lt;h4&gt;Agents will be automatically upgraded after starting management nodes&lt;/h4&gt;
  You don&#39;t need to bother with agent upgrade; they will be upgraded after management nodes reconnect them.
&lt;/div&gt;


&lt;h4&gt;6. Start UI&lt;/h4&gt;

&lt;p&gt;Now you can start the UI by &lt;code&gt;zstack-ctl start_ui&lt;/code&gt; on the local host or &lt;code&gt;zstack-ctl start_ui --host remote_host_ip&lt;/code&gt; on the remote host.&lt;/p&gt;

&lt;h4&gt;7. Upgrade Virtual Router&lt;/h4&gt;

&lt;h5&gt;Upgrade running virtual router VMs&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;select the virtual router VM&lt;/li&gt;
&lt;li&gt;click button &quot;Action&quot;&lt;/li&gt;
&lt;li&gt;click &quot;Reconnect&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;After reconnecting, the virtual router VM will be upgraded to the latest version.&lt;/p&gt;

&lt;h5&gt;Upgrade virtual router image and virtual router offering&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;add the 0.9 virtual router image http://download.zstack.org/releases/0.9/rc2/zstack-virtualrouter-0.9.0-rc.qcow2 to the backup storage:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/images/0.9/6.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;update the existing virtual router offering to the new virtual router image by CLI&lt;/p&gt;

&lt;p&gt; UpdateVirtualRouterOffering uuid=vr_offering_uuid imageUuid=new_image_uuid&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;bs-callout bs-callout-warning&quot;&gt;
  &lt;h4&gt;You cannot create new virtual router VMs if you don&#39;t upgrade the virtual offerings&lt;/h4&gt;
  If you don&#39;t upgrade existing virtual router offerings, new virtual router VMs will fail to be created because the old
  0.8(or 0.7, 0.6) image doesn&#39;t contain the agent required by the ZStack 0.9. You can use above CLI command to update existing
  offerings, or simply delete them and create new ones.
&lt;/div&gt;


&lt;h2&gt;Bug Report&lt;/h2&gt;

&lt;p&gt;If you find any bugs, please open a ticket on &lt;a href=&quot;https://github.com/zstackorg/zstack/issues&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 29 Aug 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/blog/v0.9-rc2.html</link>
        <guid isPermaLink="true">http://zstack.org/blog/v0.9-rc2.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>更改VM hostname和静态IP地址</title>
        <description>&lt;h2&gt;简介&lt;/h2&gt;

&lt;p&gt;ZStack在创建VM Instance的时候可以设定VM的hostname和静态IP地址。那么我们怎么在创建VM Instance之后修改这个云主机的hostname和IP地址呢？&lt;/p&gt;

&lt;p&gt;答案要从ZStack是如何支持云主机的hostname和静态IP地址说起。ZStack是通过特有的System Tags（系统标签）来支持这两个功能的。
&lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/tag.html#system-tags&quot;&gt;System Tags&lt;/a&gt;是ZStack特有的标签。
它的出现主要是为了解决IaaS架构稳定性的问题，保证ZStack在添加新功能的时候不必修改原有的代码。
用户在创建特定hostname和静态IP地址的云主机的时候，ZStack会创建两个特别的系统标签。ZStack对应的模块在创建云主机的过程中，
通过读取系统标签里的内容就可以把所需的内容设置上。所以，如果用户需要修改云主机的hostname和静态ip地址，
只需要修改之前的系统标签，再通过ZStack重启（不能在VM里面用reboot命令重启）云主机即可。&lt;/p&gt;

&lt;p&gt;目前ZStack的版本（0.8）里，还不支持UpdateSystemTags的API，只能通过Delete原有System Tages，再增加一个新的System Tags的方式。
ZStack 0.9版本会添加UpdateSystemTags的API。&lt;/p&gt;

&lt;h2&gt;查询System Tags&lt;/h2&gt;

&lt;p&gt;首先我们假定用户已经通过ZStack的UI界面或者zstack-cli创建了一台指定hostname和静态IP地址的云主机。&lt;/p&gt;

&lt;p&gt;例如创建了一台云主机，该云主机的hostname为vm1，静态IP地址是10.11.0.100：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;QueryVmInstance name~=vm1 
{
    &quot;inventories&quot;: [
        {
            &quot;allVolumes&quot;: [
                {
                    &quot;createDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
                    &quot;description&quot;: &quot;Root volume for VM[uuid:beda91e5c2474ab9bc5e15ce7c83de91]&quot;,
                    &quot;deviceId&quot;: 0,
                    &quot;format&quot;: &quot;qcow2&quot;,
                    &quot;installPath&quot;: &quot;/opt/zstack/nfsprimarystorage/prim-5b4d7483ba3b4d109c8d35c971fd2c24/rootVolumes/acct-36c27e8ff05c4780bf6d2fa65700f22e/vol-38d9adae6422474786d14d66672bcc9a/38d9adae6422474786d14d66672bcc9a.qcow2&quot;,
                    &quot;lastOpDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
                    &quot;name&quot;: &quot;ROOT-for-vm1&quot;,
                    &quot;primaryStorageUuid&quot;: &quot;5b4d7483ba3b4d109c8d35c971fd2c24&quot;,
                    &quot;rootImageUuid&quot;: &quot;589c8795fbdf45549be1c7a9ebdda70b&quot;,
                    &quot;size&quot;: 209715200,
                    &quot;state&quot;: &quot;Enabled&quot;,
                    &quot;status&quot;: &quot;Ready&quot;,
                    &quot;type&quot;: &quot;Root&quot;,
                    &quot;uuid&quot;: &quot;38d9adae6422474786d14d66672bcc9a&quot;,
                    &quot;vmInstanceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;
                }
            ],
            &quot;allocatorStrategy&quot;: &quot;DefaultHostAllocatorStrategy&quot;,
            &quot;clusterUuid&quot;: &quot;1fae9d050a60441eac7e15d09d7a57e0&quot;,
            &quot;cpuNum&quot;: 1,
            &quot;cpuSpeed&quot;: 512,
            &quot;createDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
            &quot;defaultL3NetworkUuid&quot;: &quot;31fd0dba47ee472481ee4edc9ab9d6ee&quot;,
            &quot;hostUuid&quot;: &quot;15d0f76c5989472e83d264a1bc408355&quot;,
            &quot;hypervisorType&quot;: &quot;KVM&quot;,
            &quot;imageUuid&quot;: &quot;589c8795fbdf45549be1c7a9ebdda70b&quot;,
            &quot;instanceOfferingUuid&quot;: &quot;db2320b776ca4365962870f371db7c5c&quot;,
            &quot;lastHostUuid&quot;: &quot;15d0f76c5989472e83d264a1bc408355&quot;,
            &quot;lastOpDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
            &quot;memorySize&quot;: 134217728,
            &quot;name&quot;: &quot;vm1&quot;,
            &quot;platform&quot;: &quot;Linux&quot;,
            &quot;rootVolumeUuid&quot;: &quot;38d9adae6422474786d14d66672bcc9a&quot;,
            &quot;state&quot;: &quot;Running&quot;,
            &quot;type&quot;: &quot;UserVm&quot;,
            &quot;uuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;,
            &quot;vmNics&quot;: [
                {
                    &quot;createDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
                    &quot;deviceId&quot;: 0,
                    &quot;gateway&quot;: &quot;10.11.0.1&quot;,
                    &quot;ip&quot;: &quot;10.11.0.100&quot;,
                    &quot;l3NetworkUuid&quot;: &quot;31fd0dba47ee472481ee4edc9ab9d6ee&quot;,
                    &quot;lastOpDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
                    &quot;mac&quot;: &quot;fa:1f:45:81:f4:00&quot;,
                    &quot;netmask&quot;: &quot;255.255.0.0&quot;,
                    &quot;uuid&quot;: &quot;c9ba9bc674084ac39a02a680ba6752fa&quot;,
                    &quot;vmInstanceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;
                }
            ],
            &quot;zoneUuid&quot;: &quot;fb92b33a29bc42a8990f5db0356493b8&quot;
        }
    ],
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;如果还没有登录，请用下面的方法登录系统：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;LogInByAccount accountName=admin password=password
{
    &quot;inventory&quot;: {
        &quot;accountUuid&quot;: &quot;36c27e8ff05c4780bf6d2fa65700f22e&quot;,
        &quot;createDate&quot;: &quot;Aug 20, 2015 7:29:33 PM&quot;,
        &quot;expiredDate&quot;: &quot;Aug 20, 2015 9:29:33 PM&quot;,
        &quot;userUuid&quot;: &quot;36c27e8ff05c4780bf6d2fa65700f22e&quot;,
        &quot;uuid&quot;: &quot;84ec1c14f1574806afe2ae2b1c963ab3&quot;
    },
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;我们可以通过查询系统标签来，得到该VM的hostname和静态IP的设置。查询vm1的System Tags，也就是查询resourceUuid为vm1的UUID的系统标签：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;QuerySystemTag resourceUuid=beda91e5c2474ab9bc5e15ce7c83de91
{
    &quot;inventories&quot;: [
        {
            &quot;createDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
            &quot;inherent&quot;: false,
            &quot;lastOpDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
            &quot;resourceType&quot;: &quot;VmInstanceVO&quot;,
            &quot;resourceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;,
            &quot;tag&quot;: &quot;staticIp::31fd0dba47ee472481ee4edc9ab9d6ee::10.11.0.100&quot;,
            &quot;type&quot;: &quot;System&quot;,
            &quot;uuid&quot;: &quot;0dc36ae8dad24409bfca5c8d307dc8d9&quot;
        },
        {
            &quot;createDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
            &quot;inherent&quot;: false,
            &quot;lastOpDate&quot;: &quot;Aug 20, 2015 5:46:16 PM&quot;,
            &quot;resourceType&quot;: &quot;VmInstanceVO&quot;,
            &quot;resourceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;,
            &quot;tag&quot;: &quot;hostname::vm1&quot;,
            &quot;type&quot;: &quot;System&quot;,
            &quot;uuid&quot;: &quot;bde1ac5f07e04e5d93849c07875ea1ff&quot;
        }
    ],
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;h2&gt;删除hostname和静态IP地址&lt;/h2&gt;

&lt;p&gt;删除hostname和静态IP地址的方法就是删除设定的系统标签。删除系统标签和删除用户普通标签（资源别名）的方法一样都是使用DeleteTag API：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;DeleteTag uuid=0dc36ae8dad24409bfca5c8d307dc8d9
{
    &quot;success&quot;: true
}

&gt;&gt;&gt;DeleteTag uuid=bde1ac5f07e04e5d93849c07875ea1ff
{
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;删除标签之后，我们将不会查询到和云主机vm1相关的标签:&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;QuerySystemTag resourceUuid=beda91e5c2474ab9bc5e15ce7c83de91
{
    &quot;inventories&quot;: [],
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;h2&gt;设置新的hostname和静态IP地址&lt;/h2&gt;

&lt;p&gt;创建系统标签的API是CreateSystemTag，这个API需要输入几个参数：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;resourceUuid： 这里就是云主机vm1的UUID&lt;/li&gt;
&lt;li&gt;resourceType: 这里的类型是VmInstanceVO , 更多类型可以访问&lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/tag.html#resource-type&quot;&gt;系统标签介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tag: 就是具体的标签。每个系统标签的定义是不同的，这个需要根据System的定义来指定。我们稍后还会看到两个不同的系统标签。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;先来创建新的hostname为newVm1：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;CreateSystemTag resourceUuid=beda91e5c2474ab9bc5e15ce7c83de91 resourceType=VmInstanceVO tag=hostname::newVm1
{
    &quot;inventory&quot;: {
        &quot;createDate&quot;: &quot;Aug 20, 2015 7:29:34 PM&quot;,
        &quot;inherent&quot;: false,
        &quot;lastOpDate&quot;: &quot;Aug 20, 2015 7:29:34 PM&quot;,
        &quot;resourceType&quot;: &quot;VmInstanceVO&quot;,
        &quot;resourceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;,
        &quot;tag&quot;: &quot;hostname::newVm1&quot;,
        &quot;type&quot;: &quot;System&quot;,
        &quot;uuid&quot;: &quot;775f6655e6604c46ad6509c0270e4249&quot;
    },
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;再来创建新的静态IP地址为10.11.0.101（需要在对应的L3的IP地址范围内）：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;CreateSystemTag resourceUuid=beda91e5c2474ab9bc5e15ce7c83de91 resourceType=VmInstanceVO tag=staticIp::31fd0dba47ee472481ee4edc9ab9d6ee::10.11.0.101
{
    &quot;inventory&quot;: {
        &quot;createDate&quot;: &quot;Aug 20, 2015 7:36:18 PM&quot;,
        &quot;inherent&quot;: false,
        &quot;lastOpDate&quot;: &quot;Aug 20, 2015 7:36:18 PM&quot;,
        &quot;resourceType&quot;: &quot;VmInstanceVO&quot;,
        &quot;resourceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;,
        &quot;tag&quot;: &quot;staticIp::31fd0dba47ee472481ee4edc9ab9d6ee::10.11.0.101&quot;,
        &quot;type&quot;: &quot;System&quot;,
        &quot;uuid&quot;: &quot;85d75b94b78f4fccb4c5ca8ab95a5e3f&quot;
    },
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;这里的31fd0dba47ee472481ee4edc9ab9d6ee是VM所在L3网络的UUID。然后让我们来重启vm1。需要注意的是，
我们需要使用StopVmInstacne 和StartVmInstance来重启VM，而不是RebootVmInstacne。&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
StopVmInstance uuid=beda91e5c2474ab9bc5e15ce7c83de91
StartVmInstance uuid=beda91e5c2474ab9bc5e15ce7c83de91
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;待vm1重新启动后，我们就会发现vm1的hostname变成了newVm1，IP地址也变成了10.11.0.101。如果是通过
UI面板重启的VM，还需要刷新一下UI面板，vm1的IP地址就会显示正常。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0.8版本中有一个bug会导致云主机在重启后，不能更改的静态IP地址。这个Bug会在0.9版本中修复&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;更新系统标签&lt;/h2&gt;

&lt;p&gt;在0.9版本之后，我们还会支持直接更新系统标签，这样可以节省更改云主机信息的步骤。更改hostname
系统标签的方法是：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
UpdateSystemTag tag=hostname::newVm1 uuid=bde1ac5f07e04e5d93849c07875ea1ff
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;这里的UUID是之前通过QuerySystemTag 查到的hostname的Tag的UUID。&lt;/p&gt;

&lt;p&gt;更改静态IP地址系统标签的方法是：
&lt;code&gt;
UpdateSystemTag tag=staticIp::31fd0dba47ee472481ee4edc9ab9d6ee::10.11.0.101 uuid=0dc36ae8dad24409bfca5c8d307dc8d9
&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;需要注意的是，这里有两个UUID。其中31fd0dba47ee472481ee4edc9ab9d6ee，
是云主机网卡所在的L3 Network的UUID。而0dc36ae8dad24409bfca5c8d307dc8d9
是之前通过QuerySystemTag API查询到的静态IP地址Tag的UUID。&lt;/p&gt;

&lt;h2&gt;添加新的网络并设置静态IP地址&lt;/h2&gt;

&lt;p&gt;在0.8版本之后，ZStack支持给云主机动态的添加网卡。通常动态添加的网卡也是一个动态的IP地址，
如果需要给动态添加的网卡提前配置一个静态的IP地址，那么我们也需要依赖静态IP地址的系统标签。&lt;/p&gt;

&lt;p&gt;我们假设需要添加的网卡所属的L3网络的UUID是：31fd0dba47ee472481ee4edc9ab9d6ee&lt;/p&gt;

&lt;p&gt;云主机的UUID是：beda91e5c2474ab9bc5e15ce7c83de91&lt;/p&gt;

&lt;p&gt;需要设定的静态IP地址是：10.11.0.102&lt;/p&gt;

&lt;p&gt;那么我们添加的系统标签的方法是：&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;
&gt;&gt;&gt;CreateSystemTag resourceUuid=beda91e5c2474ab9bc5e15ce7c83de91 resourceType=VmInstanceVO tag=staticIp::31fd0dba47ee472481ee4edc9ab9d6ee::10.11.0.102
{
    &quot;inventory&quot;: {
        &quot;createDate&quot;: &quot;Aug 20, 2015 7:36:18 PM&quot;,
        &quot;inherent&quot;: false,
        &quot;lastOpDate&quot;: &quot;Aug 20, 2015 7:36:18 PM&quot;,
        &quot;resourceType&quot;: &quot;VmInstanceVO&quot;,
        &quot;resourceUuid&quot;: &quot;beda91e5c2474ab9bc5e15ce7c83de91&quot;,
        &quot;tag&quot;: &quot;staticIp::31fd0dba47ee472481ee4edc9ab9d6ee::10.11.0.102&quot;,
        &quot;type&quot;: &quot;System&quot;,
        &quot;uuid&quot;: &quot;85d75b94b78f4fccb4c5ca8ab95a5e3f&quot;
    },
    &quot;success&quot;: true
}
&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;在添加完系统标签后，我们再添加网卡的时候，ZStack就会使用刚刚设置的静态IP地址了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://zstackdoc.readthedocs.org/en/latest/userManual/tag.html#system-tags&quot;&gt;系统标签手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;./v0.7.html&quot;&gt;静态IP地址功能介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;./attach-detach-l3-tutorials.html&quot;&gt;给云主机动态的添加删除网卡&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 22 Aug 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn_blog/update-system-tags-by-delete-add.html</link>
        <guid isPermaLink="true">http://zstack.org/cn_blog/update-system-tags-by-delete-add.html</guid>
        
        
        <category>cn_blog</category>
        
      </item>
    
      <item>
        <title>ZStack 2015 技术分享会【上海站】</title>
        <description>&lt;p&gt;&lt;strong&gt;“用最纯粹的技术回馈IaaS社区”&lt;/strong&gt;，是整个ZStack团队的信仰。ZStack开发团队将计划借0.9版本发布之际，首次在国内举办几场技术交流会。
继&lt;a href=&quot;./zstack-meetup-beijing-2015.html&quot;&gt;北京站&lt;/a&gt;开始报名后，上海站也公布了报名的详细情况。
上海站交流会会于9月12号周六下午1点在上海长宁区福泉路111号上海神州数码有限公司1楼大视频会议室举行。
为保证技术交流的质量，我们设置了活动报名人数的上限为50人，请关心ZStack现状和发展的开发者和用户们务必提前报名。&lt;/p&gt;

&lt;p&gt;本次研讨会的主题包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ZStack技术架构详解&lt;/li&gt;
&lt;li&gt;ZStack实战分享&lt;/li&gt;
&lt;li&gt;现场互动问答&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;ZStack （zstack.org） 是全新一代开源IaaS，具有快速安装，快速部署，易维护，易升级，高性能等特点。
自2015年4月公开发布0.6版本后，陆续发布了0.7和0.8版本，9月初前后会发布支持ceph的 0.9版本。
目前ZStack的功能已经覆盖了大部分的企业私有云应用场景，国内外也有不少用户在测试使用。ZStack的首批客户也于7、8月份陆续部署上线运行。
对于希望使用IaaS软件，但是由于种种技术上的原因还没有成功部署私有云的企业来说，ZStack提供了全新的选择和云端体验。&lt;/p&gt;

&lt;p&gt;本次上海站技术交流会，我们要特别感谢&lt;strong&gt;上海神州数码有限公司&lt;/strong&gt;提供场地！&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;大家可以扫描下面的二维码报名ZStack技术交流会：&lt;/strong&gt;
&lt;img src=&quot;/images/meetups/2015/shanghai/registeration.png&quot; class=&quot;center-img img-responsive&quot;&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Aug 2015 00:00:00 +0800</pubDate>
        <link>http://zstack.org/cn_blog/zstack-meetup-shanghai-2015.html</link>
        <guid isPermaLink="true">http://zstack.org/cn_blog/zstack-meetup-shanghai-2015.html</guid>
        
        
        <category>cn_blog</category>
        
      </item>
    
  </channel>
</rss>
